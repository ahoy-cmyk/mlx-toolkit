#!/usr/bin/env python3
"""Main CLI for MLX Toolkit"""

import click
import os
import sys
import subprocess
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table
from rich.layout import Layout
from rich.live import Live
from rich.align import Align
from rich.columns import Columns
from rich.tree import Tree
from rich.status import Status
import psutil
import time
import threading
import json
import statistics
from datetime import datetime, timedelta
import math

from .project_manager import ProjectManager
from .package_manager import PackageManager
from .model_manager import ModelManager

console = Console()


def ensure_project_env():
    """Ensure we're in a project directory with a virtual environment"""
    if not Path("venv").exists():
        console.print("[red]‚ùå Not in an MLX project directory![/]")
        console.print("[yellow]Run this command from inside a project created with:[/]")
        console.print("  mlx-toolkit init <project-name>")
        console.print("  cd <project-name>")
        sys.exit(1)
    return True


def run_in_project_env(command_args):
    """Run a command in the project's virtual environment"""
    ensure_project_env()
    
    # Activate venv and run command
    if os.name == 'nt':  # Windows
        python_path = "venv/Scripts/python"
    else:  # Unix/macOS
        python_path = "venv/bin/python"
    
    # Run the command in the project's environment
    result = subprocess.run([python_path] + command_args, cwd=os.getcwd())
    return result.returncode == 0


@click.group()
@click.version_option(version="0.1.0")
def main():
    """üöÄ MLX Toolkit - Beautiful CLI for LLM development on Apple Silicon
    
    üéØ Core Commands:
      init <name>           Create new project with isolated environment
      install-deps          Install all MLX packages automatically  
      status                Beautiful system dashboard (try --live!)
      models                Browse and manage models with search
      download-model        Download models from Hugging Face
      lora <model> <data>   LoRA fine-tune quantized models  
      chat <model>          Colorized chat interface
    
    üî• NEW: Performance & Monitoring:
      train-monitor         Live training dashboard with ASCII charts!
      benchmark <model>     Comprehensive model performance testing
      checkpoints           Browse and manage training checkpoints
      debug-checkpoints     Debug checkpoint detection (troubleshooting)
    
    Perfect for M1/M2/M3/M4 Macs with 8GB+ RAM
    """
    pass


@main.command()
@click.argument("project_name")
@click.option("--python", default="python3", help="Python interpreter to use")
@click.option("--template", type=click.Choice(["basic", "advanced"]), default="basic")
def init(project_name, python, template):
    """Initialize a new MLX project with virtual environment"""
    manager = ProjectManager()
    
    with console.status(f"[bold green]Creating project {project_name}..."):
        success = manager.create_project(project_name, python, template)
    
    if success:
        console.print(Panel(
            f"[bold green]‚úì Project '{project_name}' created successfully![/]\n\n"
            f"[yellow]Next steps:[/]\n"
            f"1. cd {project_name}\n"
            f"2. source venv/bin/activate\n"
            f"3. mlx-toolkit install-deps",
            title="Project Created",
            border_style="green"
        ))
    else:
        console.print("[bold red]Failed to create project[/]")
        sys.exit(1)


@main.command()
@click.option("--dev", is_flag=True, help="Install development dependencies")
def install_deps(dev):
    """Install MLX and related packages in current project"""
    ensure_project_env()
    
    # Install packages using the project's virtual environment
    if os.name == 'nt':  # Windows
        pip_path = "venv/Scripts/pip"
        python_path = "venv/Scripts/python"
    else:  # Unix/macOS
        pip_path = "venv/bin/pip"
        python_path = "venv/bin/python"
    
    # Core MLX packages
    packages = [
        "mlx", "mlx-lm", "huggingface-hub", "sentencepiece",
        "numpy", "transformers", "datasets", "tokenizers", 
        "safetensors", "tqdm", "pyyaml", "pandas",
        "matplotlib", "seaborn", "jupyter", "ipykernel",
        "accelerate", "protobuf", "fire", "torch", "rich",
        "psutil>=5.8.0"  # Required for system monitoring
    ]
    
    if dev:
        packages.extend([
            "pytest", "pytest-cov", "black", "flake8", 
            "mypy", "pre-commit", "ipdb", "wandb", "tensorboard"
        ])
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task("Installing MLX packages in project environment...", total=None)
        
        try:
            # Upgrade pip first
            subprocess.run([python_path, "-m", "pip", "install", "--upgrade", "pip"], check=True)
            
            # Install packages in batches
            batch_size = 5
            for i in range(0, len(packages), batch_size):
                batch = packages[i:i+batch_size]
                subprocess.run([pip_path, "install"] + batch, check=True)
            
            progress.update(task, completed=True)
            console.print("[bold green]‚úì MLX packages installed in project environment![/]")
            
        except subprocess.CalledProcessError as e:
            console.print(f"[bold red]Failed to install packages: {e}[/]")
            sys.exit(1)


@main.command()
@click.argument("model_id")
@click.option("--output", "-o", default="models", help="Output directory for model")
def download_model(model_id, output):
    """Download a model from Hugging Face (auto-handles project environment)"""
    ensure_project_env()
    
    with console.status(f"[bold blue]Downloading {model_id} to project..."):
        # Use project's Python environment
        if os.name == 'nt':
            python_path = "venv/Scripts/python"
        else:
            python_path = "venv/bin/python"
        
        try:
            # Check if it's an MLX community model (already converted)
            if "mlx-community" in model_id.lower():
                # Use huggingface-hub to download pre-converted MLX models
                # Safely pass the arguments  
                model_name = model_id.split("/")[-1]
                cmd = [
                    python_path, "-c",
                    f"""
import sys
import os
from pathlib import Path
sys.path.insert(0, '.')
try:
    from huggingface_hub import snapshot_download
    
    # Create output directory
    output_path = Path('{output}') / '{model_name}'
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f'Downloading {model_id} to {{output_path}}...')
    
    # Download the pre-converted MLX model (get all files including safetensors)
    snapshot_download(
        repo_id='{model_id}',
        local_dir=str(output_path),
        # Don't ignore safetensors - MLX models need them
    )
    
    # Verify the download
    safetensor_files = list(output_path.glob('*.safetensors'))
    if safetensor_files:
        print(f'‚úì MLX model downloaded successfully to {{{{output_path}}}}!')
        print(f'Found {{{{len(safetensor_files)}}}} safetensors files')
    else:
        print(f'‚ö† Warning: No safetensors files found. This might not be a valid MLX model.')
        
except Exception as e:
    print(f'Download failed: {{{{e}}}}')
    import traceback
    traceback.print_exc()
    sys.exit(1)
"""
                ]
            else:
                # Regular HF model - convert using mlx_lm
                cmd = [
                    python_path, "-c",
                    f"""
import sys
sys.path.insert(0, '.')
try:
    from mlx_lm import convert
    convert.convert('{model_id}', mlx_path='{output}/{model_id.split("/")[-1]}', quantize=True)
    print('Model downloaded and converted successfully!')
except Exception as e:
    print(f'Download failed: {{{{e}}}}')
    sys.exit(1)
"""
                ]
            
            # Run with real-time output instead of capturing
            result = subprocess.run(cmd, check=True)
            console.print(f"[bold green]‚úì Model ready at {output}/{model_id.split('/')[-1]}[/]")
            
        except subprocess.CalledProcessError as e:
            console.print(f"[bold red]Failed to download model[/]")
            console.print("[yellow]Make sure you've run 'mlx-toolkit install-deps' first[/]")
            if "mlx-community" in model_id.lower():
                console.print("[cyan]Note: MLX community models are pre-converted and ready to use[/]")
                console.print("[cyan]Try: huggingface-cli login (if you need authentication)[/]")
            sys.exit(1)


@main.command()
@click.argument("model_path")
@click.argument("data_path")
@click.option("--output", "-o", default="models/lora_adapters", help="Output path for LoRA adapters")
@click.option("--iters", default=1000, help="Number of training iterations")
@click.option("--learning-rate", default=1e-5, help="Learning rate")
@click.option("--lora-rank", default=8, help="LoRA rank")
@click.option("--save-every", default=100, help="Save checkpoint every N steps")
@click.option("--batch-size", default=4, help="Batch size for training")
@click.option("--resume/--no-resume", default=True, help="Automatically resume from checkpoint if available")
def lora(model_path, data_path, output, iters, learning_rate, lora_rank, save_every, batch_size, resume):
    """Fine-tune using mlx_lm.lora with automatic checkpoint resumption"""
    ensure_project_env()
    import subprocess
    
    console.print(Panel(
        f"[bold green]üöÄ MLX-LM LoRA Fine-tuning[/]\n\n"
        f"Model: {model_path}\n"
        f"Data: {data_path}\n"
        f"Output: {output}\n"
        f"Iterations: {iters} | Learning Rate: {learning_rate}\n"
        f"LoRA Rank: {lora_rank} | Batch Size: {batch_size}\n"
        f"Save Every: {save_every} steps",
        title="LoRA Training Setup",
        border_style="green"
    ))
    
    # Validate inputs
    if not Path(model_path).exists():
        console.print(f"[red]‚ùå Model not found: {model_path}[/]")
        sys.exit(1)
    
    if not Path(data_path).exists():
        console.print(f"[red]‚ùå Training data not found: {data_path}[/]")
        sys.exit(1)
    
    # Check and convert data format for mlx_lm compatibility
    console.print("[yellow]Preparing data for mlx_lm.lora...[/]")
    
    # Create a proper train.jsonl in the format mlx_lm expects
    import json
    converted_count = 0
    
    try:
        # First, check what format the data is in
        with open(data_path, 'r') as f:
            first_line = f.readline().strip()
            if first_line:
                sample_data = json.loads(first_line)
                console.print(f"[dim]Sample data format: {list(sample_data.keys())}[/]")
        
        # Always create a new converted file for mlx_lm
        mlx_lm_data_path = "train.jsonl"  # mlx_lm expects this specific name
        
        with open(data_path, 'r') as infile, open(mlx_lm_data_path, 'w') as outfile:
            for line_num, line in enumerate(infile, 1):
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        
                        # Convert to mlx_lm format with clean JSON
                        if 'instruction' in data and 'response' in data:
                            # Clean the text content - remove any problematic characters
                            instruction = str(data['instruction']).replace('\n', ' ').replace('\r', ' ').strip()
                            response = str(data['response']).replace('\n', ' ').replace('\r', ' ').strip()
                            
                            # Simple text format that mlx_lm can handle
                            text_content = f"User: {instruction} Assistant: {response}"
                            mlx_lm_entry = {"text": text_content}
                        elif 'text' in data:
                            # Clean existing text format
                            clean_text = str(data['text']).replace('\n', ' ').replace('\r', ' ').strip()
                            mlx_lm_entry = {"text": clean_text}
                        else:
                            console.print(f"[yellow]Skipping line {line_num}: unknown format[/]")
                            continue
                        
                        # Write clean JSON - ensure no extra characters
                        json_line = json.dumps(mlx_lm_entry, ensure_ascii=True)
                        outfile.write(json_line + '\n')
                        converted_count += 1
                        
                    except json.JSONDecodeError as e:
                        console.print(f"[yellow]Skipping invalid JSON on line {line_num}: {e}[/]")
                        continue
        
        if converted_count > 0:
            console.print(f"[green]‚úì Converted {converted_count} examples to {mlx_lm_data_path}[/]")
            data_path = mlx_lm_data_path
        else:
            console.print("[red]‚ùå No valid training examples found![/]")
            sys.exit(1)
            
    except Exception as e:
        console.print(f"[red]‚ùå Data preparation failed: {e}[/]")
        sys.exit(1)
    
    # Create output directory
    Path(output).parent.mkdir(parents=True, exist_ok=True)
    
    # Check if mlx-lm is installed in project environment
    if os.name == 'nt':  # Windows
        python_path = "venv/Scripts/python"
        pip_path = "venv/Scripts/pip"
    else:  # Unix/macOS
        python_path = "venv/bin/python"
        pip_path = "venv/bin/pip"
    
    # Install mlx-lm if not available
    console.print("[yellow]Checking mlx-lm installation...[/]")
    try:
        check_result = subprocess.run([python_path, "-c", "import mlx_lm"], 
                                    capture_output=True, cwd=os.getcwd())
        if check_result.returncode != 0:
            console.print("[yellow]Installing mlx-lm in project environment...[/]")
            install_result = subprocess.run([pip_path, "install", "mlx-lm"], cwd=os.getcwd())
            if install_result.returncode != 0:
                console.print("[red]‚ùå Failed to install mlx-lm[/]")
                return
    except Exception as e:
        console.print(f"[red]‚ùå Error checking mlx-lm: {e}[/]")
        return
    
    # Run mlx_lm.lora command in project environment
    console.print("[bold yellow]üöÄ Starting mlx_lm.lora training...[/]")
    
    try:
        # Create data directory structure that mlx_lm expects
        data_dir = "mlx_data"
        Path(data_dir).mkdir(exist_ok=True)
        
        # Split data into train/validation and copy to expected locations
        import shutil
        train_file = Path(data_dir) / "train.jsonl"
        valid_file = Path(data_dir) / "valid.jsonl"
        
        # Read all converted data
        all_data = []
        with open(data_path, 'r') as f:
            for line in f:
                if line.strip():
                    all_data.append(line.strip())
        
        # Split data: 90% train, 10% validation (minimum 1 validation example)
        if len(all_data) < 2:
            console.print(f"[yellow]‚ö†Ô∏è  Only {len(all_data)} examples - using all for training and validation[/]")
            train_data = all_data
            valid_data = all_data  # Use same data for validation if too few examples
        else:
            split_idx = max(1, int(len(all_data) * 0.9))  # At least 1 validation example
            train_data = all_data[:split_idx]
            valid_data = all_data[split_idx:]
        
        # Write train data
        with open(train_file, 'w') as f:
            for line in train_data:
                f.write(line + '\n')
        
        # Write validation data
        with open(valid_file, 'w') as f:
            for line in valid_data:
                f.write(line + '\n')
        
        console.print(f"[green]‚úì Data prepared: {len(train_data)} train, {len(valid_data)} validation examples[/]")
        
        # üî• EPIC CHECKPOINT DETECTION AND RESUMPTION
        checkpoint_info = None
        if resume:
            checkpoint_info = _detect_and_handle_checkpoints(output, iters, model_path, data_path)
        
        # Determine actual iterations to run (adjust for resumption)
        actual_iters = iters
        if checkpoint_info and checkpoint_info.get('remaining'):
            actual_iters = checkpoint_info['remaining']
            console.print(f"[yellow]üîß Adjusting iterations: {iters} total ‚Üí {actual_iters} remaining[/]")
        
        # Build command with optional resumption
        cmd = [
            python_path, "-m", "mlx_lm", "lora",
            "--model", model_path,
            "--train",
            "--data", data_dir,  # Use directory, not file
            "--iters", str(actual_iters),
            "--save-every", str(save_every),
            "--adapter-path", output,
            "--learning-rate", str(learning_rate),
            "--batch-size", str(batch_size),
            "--steps-per-report", "10",
            "--steps-per-eval", "100"
        ]
        
        # Add resumption from checkpoint if detected
        if checkpoint_info and checkpoint_info.get('resume_from'):
            # MLX-LM uses --adapter-path for both input and output, so we need to check what it expects
            # Let's try the most common MLX-LM resumption patterns
            resume_file = checkpoint_info['resume_from']
            console.print(f"[bold green]üîÑ Resuming from checkpoint: {checkpoint_info['checkpoint_name']}[/]")
            console.print(f"[dim]üìÅ Resume file: {resume_file}[/]")
            
            # Try different MLX-LM resumption approaches
            if "adapters" in resume_file:
                # Method 1: --resume-adapter-file (if supported)
                cmd.extend(["--resume-adapter-file", resume_file])
            else:
                # Method 2: Point --adapter-path to the checkpoint directory
                checkpoint_dir = str(Path(resume_file).parent)
                console.print(f"[dim]üìÇ Using checkpoint directory: {checkpoint_dir}[/]")
                # Replace the output adapter path with the existing one for resumption
                cmd = [c if c != output else checkpoint_dir for c in cmd]
            
            console.print(f"[dim]üîß Modified command for resumption[/]")
        
        console.print(f"[dim]Running: {' '.join(cmd)}[/]")
        console.print(f"[dim]Logging to: training.log[/]")
        
        # Run with output logging for monitoring
        with open("training.log", "w") as log_file:
            log_file.write(f"MLX-LM LoRA Training Started: {datetime.now()}\n")
            log_file.write(f"Command: {' '.join(cmd)}\n")
            log_file.write("=" * 50 + "\n")
            log_file.flush()
            
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1,
                cwd=os.getcwd()
            )
            
            # Stream output to both console and log file
            for line in process.stdout:
                print(line.rstrip())  # Show in console
                log_file.write(line)  # Save to log
                log_file.flush()
            
            process.wait()
            result_code = process.returncode
            log_file.write(f"\nTraining completed with exit code: {result_code}\n")
        
        console.print(f"\n[cyan]üí° Monitor training progress: mlx-toolkit train-monitor --live[/]")
        
        if result_code == 0:
            console.print(Panel(
                f"[bold green]‚úÖ LoRA fine-tuning completed![/]\n\n"
                f"[yellow]Your LoRA adapters are saved to:[/] {output}\n\n"
                f"[cyan]üí¨ To chat with your model:[/]\n"
                f"[white]mlx-toolkit chat {model_path} --adapter-path {output}[/]\n\n"
                f"[cyan]üîç To generate text:[/]\n"
                f"[white]python -m mlx_lm generate --model {model_path} --adapter-path {output} --prompt 'Your question'[/]",
                title="üéâ Training Complete",
                border_style="green"
            ))
        else:
            console.print(f"[red]‚ùå LoRA training failed with exit code {result_code}[/]")
            
    except Exception as e:
        console.print(f"[red]‚ùå Error running mlx_lm.lora: {e}[/]")
        console.print("[yellow]üí° Make sure mlx-lm is installed: pip install mlx-lm[/]")


@main.command()
@click.argument("model_path")
@click.option("--adapter-path", help="Path to LoRA adapter (for fine-tuned models)")
@click.option("--prompt", "-p", help="Single prompt instead of interactive chat")
@click.option("--max-tokens", default=512, help="Maximum tokens to generate")
@click.option("--temperature", default=0.7, help="Temperature for generation")
def chat(model_path, adapter_path, prompt, max_tokens, temperature):
    """Chat with a model (supports LoRA adapters)"""
    ensure_project_env()
    
    # Validate model path
    if not Path(model_path).exists():
        console.print(f"[red]‚ùå Model not found: {model_path}[/]")
        sys.exit(1)
    
    # Check if mlx-lm is available
    if os.name == 'nt':  # Windows
        python_path = "venv/Scripts/python"
    else:  # Unix/macOS
        python_path = "venv/bin/python"
    
    if prompt:
        # Single generation mode
        cmd = [python_path, "-m", "mlx_lm", "generate", "--model", model_path]
        if adapter_path:
            cmd.extend(["--adapter-path", adapter_path])
        cmd.extend([
            "--prompt", prompt,
            "--max-tokens", str(max_tokens),
            "--temp", str(temperature)
        ])
        console.print(f"[yellow]ü§ñ Generating response...[/]")
        
        try:
            result = subprocess.run(cmd, cwd=os.getcwd())
            if result.returncode != 0:
                console.print(f"[red]‚ùå Generation failed with exit code {result.returncode}[/]")
        except Exception as e:
            console.print(f"[red]‚ùå Error generating response: {e}[/]")
            console.print("[yellow]üí° Make sure mlx-lm is installed: pip install mlx-lm[/]")
    else:
        # Custom interactive chat with colors
        _run_colorized_chat(python_path, model_path, adapter_path, max_tokens, temperature)


def _run_colorized_chat(python_path, model_path, adapter_path, max_tokens, temperature):
    """Run an interactive chat session with colorized output"""
    import json
    from rich.prompt import Prompt
    from rich.live import Live
    from rich.text import Text
    
    # Display chat session info
    console.print(Panel(
        f"[bold cyan]üöÄ MLX Chat Session[/]\n\n"
        f"Model: {model_path}\n" +
        (f"Adapter: {adapter_path}\n" if adapter_path else "") +
        f"Max Tokens: {max_tokens} | Temperature: {temperature}\n\n"
        f"[bold yellow]üí¨ Chat Commands:[/]\n"
        f"[dim]  /help - Show commands\n"
        f"  /clear - Clear conversation\n"
        f"  /quit - Exit chat\n"
        f"  Just type to chat![/]",
        title="Chat Ready",
        border_style="cyan"
    ))
    
    conversation_history = []
    
    while True:
        try:
            # Get user input with colored prompt
            user_input = Prompt.ask("\n[bold blue]üë§ You[/]")
            
            if not user_input.strip():
                continue
                
            # Handle commands
            if user_input.startswith("/"):
                if user_input.lower() in ["/quit", "/exit", "/q"]:
                    console.print("[yellow]üëã Chat session ended[/]")
                    break
                elif user_input.lower() in ["/clear", "/c"]:
                    conversation_history.clear()
                    console.print("[green]üßπ Conversation cleared[/]")
                    continue
                elif user_input.lower() in ["/help", "/h"]:
                    console.print(Panel(
                        "[bold]Chat Commands:[/]\n\n"
                        "[cyan]/help, /h[/] - Show this help\n"
                        "[cyan]/clear, /c[/] - Clear conversation history\n"
                        "[cyan]/quit, /q[/] - Exit chat\n\n"
                        "[dim]Just type your message to chat with the model![/]",
                        title="Help",
                        border_style="blue"
                    ))
                    continue
                else:
                    console.print("[red]‚ùå Unknown command. Type /help for available commands.[/]")
                    continue
            
            # Add to conversation history
            conversation_history.append({"role": "user", "content": user_input})
            
            # Prepare the full conversation context
            if len(conversation_history) > 10:  # Keep last 10 exchanges
                context_history = conversation_history[-10:]
            else:
                context_history = conversation_history
            
            # Build the prompt with conversation context
            full_prompt = ""
            for msg in context_history:
                if msg["role"] == "user":
                    full_prompt += f"User: {msg['content']}\n"
                else:
                    full_prompt += f"Assistant: {msg['content']}\n"
            full_prompt += "Assistant:"
            
            # Build generation command
            cmd = [python_path, "-m", "mlx_lm", "generate", "--model", model_path]
            if adapter_path:
                cmd.extend(["--adapter-path", adapter_path])
            cmd.extend([
                "--prompt", full_prompt,
                "--max-tokens", str(max_tokens),
                "--temp", str(temperature)
            ])
            
            # Show that we're generating
            console.print("\n[bold green]ü§ñ Assistant[/]: ", end="")
            
            try:
                # Run generation and capture output
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    cwd=os.getcwd()
                )
                
                if result.returncode == 0:
                    # Extract the response (remove the prompt part)
                    response = result.stdout.strip()
                    if response:
                        # Clean up the response - remove the original prompt if it's echoed back
                        if "Assistant:" in response:
                            response = response.split("Assistant:")[-1].strip()
                        
                        # Display the response with streaming effect
                        response_text = Text(response)
                        response_text.stylize("green")
                        console.print(response_text)
                        
                        # Add assistant response to history
                        conversation_history.append({"role": "assistant", "content": response})
                    else:
                        console.print("[red]‚ùå No response generated[/]")
                else:
                    console.print(f"[red]‚ùå Generation failed: {result.stderr}[/]")
                    
            except Exception as e:
                console.print(f"[red]‚ùå Error during generation: {e}[/]")
                
        except KeyboardInterrupt:
            console.print("\n[yellow]üëã Chat session ended[/]")
            break
        except EOFError:
            console.print("\n[yellow]üëã Chat session ended[/]")
            break


@main.command()
@click.argument("model_path")
@click.argument("data_path")
@click.option("--output", "-o", default="fine_tuned_model", help="Output path for fine-tuned model")
@click.option("--epochs", default=3, help="Number of training epochs")
@click.option("--batch-size", default=4, help="Batch size for training")
@click.option("--learning-rate", default=1e-5, help="Learning rate")
@click.option("--lora-rank", default=8, help="LoRA rank")
@click.option("--lora-alpha", default=16, help="LoRA alpha")
@click.option("--max-seq-length", default=2048, help="Maximum sequence length")
@click.option("--save-every", default=100, help="Save checkpoint every N steps")
@click.option("--eval-every", default=50, help="Evaluate every N steps")
@click.option("--warmup-steps", default=10, help="Warmup steps")
def finetune(model_path, data_path, output, epochs, batch_size, learning_rate, 
             lora_rank, lora_alpha, max_seq_length, save_every, eval_every, warmup_steps):
    """Fine-tune a model using MLX LoRA"""
    ensure_project_env()
    
    # Validate inputs
    if not Path(model_path).exists():
        console.print(f"[red]‚ùå Model not found: {model_path}[/]")
        sys.exit(1)
    
    if not Path(data_path).exists():
        console.print(f"[red]‚ùå Training data not found: {data_path}[/]")
        sys.exit(1)
    
    # Special handling for quantized models
    model_name = str(model_path)
    if "4bit" in model_name or "8bit" in model_name or "quantized" in model_name:
        console.print(Panel(
            "[yellow]‚ö†Ô∏è Quantized model detected![/]\n\n"
            "[cyan]For best results with quantized models, use mlx_lm directly:[/]\n\n"
            "[white]# Install mlx_lm with LoRA support[/]\n"
            "[green]pip install mlx-lm\n\n[/]"
            "[white]# Run LoRA fine-tuning[/]\n"
            f"[green]python -m mlx_lm.lora \\\n"
            f"  --model {model_path} \\\n"
            f"  --train \\\n"
            f"  --data {data_path} \\\n"
            f"  --iters 1000 \\\n"
            f"  --save-every 100 \\\n"
            f"  --adapter-path {output}[/]\n\n"
            "[cyan]Press Ctrl+C to stop and use mlx_lm.lora instead, or[/]\n"
            "[yellow]Continue with MLX Toolkit evaluation mode...[/]",
            title="üí° Quantized Model Tips",
            border_style="yellow"
        ))
        
        import time
        time.sleep(2)  # Give user time to read
    
    console.print(Panel(
        f"[bold yellow]LoRA Fine-tuning Configuration:[/]\n"
        f"Model: {model_path}\n"
        f"Data: {data_path}\n"
        f"Output: {output}\n"
        f"Epochs: {epochs} | Batch Size: {batch_size}\n"
        f"Learning Rate: {learning_rate}\n"
        f"LoRA Rank: {lora_rank} | LoRA Alpha: {lora_alpha}\n"
        f"Max Seq Length: {max_seq_length}",
        title="Fine-tuning Setup",
        border_style="yellow"
    ))
    
    if os.name == 'nt':
        python_path = "venv/Scripts/python"
    else:
        python_path = "venv/bin/python"
    
    # Create fine-tuning script
    finetune_script = f"""
import sys
import json
import math
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Any, Optional

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from mlx_lm import load
import json
from pathlib import Path
from transformers import AutoTokenizer
try:
    # Try to import LoRA functionality if available
    from mlx_lm import lora
    LORA_AVAILABLE = True
except ImportError:
    LORA_AVAILABLE = False

def save_model_and_tokenizer(output_path: str, model, tokenizer):
    \"\"\"Save model weights and tokenizer to the specified path\"\"\"
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    try:
        # Save model weights using MLX native method
        model.save_weights(str(output_path / "model.npz"))
        
        # Save tokenizer
        tokenizer.save_pretrained(str(output_path))
        
        # Save a basic config file
        config_data = {{
            "model_type": "mlx_lora_model",
            "saved_with": "mlx_toolkit"
        }}
        with open(output_path / "config.json", "w") as f:
            json.dump(config_data, f, indent=2)
        
        return True
    except Exception as e:
        print(f"Error saving model: {{e}}")
        return False

@dataclass
class LoRAConfig:
    rank: int = {lora_rank}
    alpha: float = {lora_alpha}
    dropout: float = 0.0
    scale: float = {lora_alpha / lora_rank}

def load_data(data_path: str):
    \"\"\"Load training data from JSONL file\"\"\"
    data = []
    try:
        with open(data_path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    item = json.loads(line)
                    if 'text' in item:
                        data.append(item['text'])
                    elif 'instruction' in item and 'response' in item:
                        # Convert instruction-response format
                        text = f"### Instruction: {{item['instruction']}}\\n### Response: {{item['response']}}"
                        data.append(text)
                    else:
                        print(f"Warning: Line {{line_num}} missing 'text' or 'instruction'/'response' fields")
                except json.JSONDecodeError as e:
                    print(f"Error parsing line {{line_num}}: {{e}}")
                    continue
        
        if not data:
            raise ValueError("No valid training data found")
        
        print(f"Loaded {{len(data)}} training examples")
        return data
        
    except Exception as e:
        print(f"Error loading data from {{data_path}}: {{e}}")
        sys.exit(1)

def tokenize_data(tokenizer, data, max_length={max_seq_length}):
    \"\"\"Tokenize training data\"\"\"
    print(f"Tokenizing {{len(data)}} examples...")
    
    tokenized = []
    for i, text in enumerate(data):
        if i % 100 == 0:
            print(f"Tokenizing {{i}}/{{len(data)}}...")
        
        # Tokenize using encode method which is available on TokenizerWrapper
        # For MLX TokenizerWrapper, we need to use explicit methods rather than calling directly
        try:
            # First try the standard HuggingFace approach
            input_ids = tokenizer.encode(
                text,
                max_length=max_length,
                truncation=True,
                padding=False,
                return_overflowing_tokens=False,
                return_special_tokens_mask=False,
                return_offsets_mapping=False,
                return_length=False,
                verbose=False
            )
            tokens = {{"input_ids": [input_ids]}}
        except Exception as e:
            # Fallback: Try calling the tokenizer as a function (may work with newer versions)
            try:
                tokens = tokenizer(
                    text,
                    max_length=max_length,
                    truncation=True,
                    padding=False,
                    return_tensors="np"
                )
            except Exception as e2:
                print(f"Error tokenizing text: {{e2}}")
                continue
        
        input_ids = tokens["input_ids"][0]
        if len(input_ids) > 10:  # Skip very short sequences
            tokenized.append(mx.array(input_ids))
    
    print(f"‚úì Tokenized {{len(tokenized)}} valid sequences")
    return tokenized

def create_batches(data, batch_size):
    \"\"\"Create training batches\"\"\"
    for i in range(0, len(data), {batch_size}):
        batch = data[i:i + {batch_size}]
        if len(batch) == {batch_size}:  # Only full batches
            yield batch

def compute_loss(model, batch):
    \"\"\"Compute language modeling loss\"\"\"
    # Simple next-token prediction loss
    losses = []
    for sequence in batch:
        if len(sequence) < 2:
            continue
            
        inputs = sequence[:-1]
        targets = sequence[1:]
        
        logits = model(inputs[None])  # Add batch dimension
        
        # Compute cross-entropy loss
        loss = nn.losses.cross_entropy(logits[0], targets)
        losses.append(loss)
    
    if losses:
        # Compute mean of scalar losses
        total_loss = mx.array(0.0)
        for loss in losses:
            total_loss = total_loss + mx.mean(loss)  # Ensure scalar
        return total_loss / len(losses)
    else:
        return mx.array(0.0)

def train_step(model, optimizer, batch):
    \"\"\"Single training step\"\"\"
    def loss_fn(model):
        return compute_loss(model, batch)
    
    try:
        loss, grads = mx.value_and_grad(loss_fn)(model)
        optimizer.update(model, grads)
        mx.eval(model.parameters(), optimizer.state)
        return loss
    except RuntimeError as e:
        if \"quantized weights\" in str(e):
            # Only print warning once per epoch
            if not hasattr(train_step, 'quantized_warned'):
                print(f\"\\n‚ö†Ô∏è Quantized model - using loss monitoring (no weight updates)\")
                train_step.quantized_warned = True
            
            # For quantized models, we can still compute loss for monitoring
            loss = compute_loss(model, batch)
            return loss
        else:
            raise e

def main():
    print("üöÄ Starting LoRA Fine-tuning...")
    
    # Load model and tokenizer
    print("Loading model from {model_path}...")
    try:
        model, tokenizer = load("{model_path}")
        print("‚úì Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Error loading model: {{e}}")
        sys.exit(1)
    
    # Load and tokenize data
    train_data = load_data("{data_path}")
    tokenized_data = tokenize_data(tokenizer, train_data)
    
    if len(tokenized_data) < {batch_size}:
        print(f"‚ùå Not enough data for batch size {{batch_size}}. Need at least {{batch_size}} sequences.")
        sys.exit(1)
    
    # Setup LoRA (simplified approach)
    print("Setting up LoRA (rank={lora_rank}, alpha={lora_alpha})...")
    
    # For quantized models, we'll use a monitoring approach
    # since direct gradient computation on quantized weights is not supported
    if LORA_AVAILABLE:
        print("‚úì MLX-LM LoRA module available")
    else:
        print("‚ö†Ô∏è MLX-LM LoRA not available, using evaluation mode")
    
    print("‚úì Model ready for training/evaluation")
    print("‚úì Quantized weights will be preserved")
    
    # Setup optimizer
    optimizer = optim.AdamW(learning_rate={learning_rate})
    
    # Training loop
    print("\\nüìö Starting training for {epochs} epochs...")
    
    total_steps = 0
    for epoch in range({epochs}):
        print(f"\\nüîÑ Epoch {{epoch + 1}}/{epochs}")
        
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(create_batches(tokenized_data, {batch_size})):
            # Training step
            loss = train_step(model, optimizer, batch)
            
            epoch_loss += loss.item()
            num_batches += 1
            total_steps += 1
            
            if batch_idx % 10 == 0:
                print(f"  Step {{total_steps}}: Loss = {{loss.item():.4f}}")
            
            # Save checkpoint
            if total_steps % {save_every} == 0:
                checkpoint_path = Path("{output}") / f"checkpoint-{{total_steps}}"
                checkpoint_path.mkdir(parents=True, exist_ok=True)
                try:
                    if save_model_and_tokenizer(str(checkpoint_path), model, tokenizer):
                        print(f"  üíæ Checkpoint saved: {{checkpoint_path}}")
                    else:
                        print(f"  ‚ö†Ô∏è Failed to save checkpoint")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Failed to save checkpoint: {{e}}")
        
        if num_batches > 0:
            avg_loss = epoch_loss / num_batches
            print(f"‚úì Epoch {{epoch + 1}} completed - Average Loss: {{avg_loss:.4f}}")
        else:
            print(f"‚ö†Ô∏è No batches processed in epoch {{epoch + 1}}")
    
    # Save final model
    print("\\nüíæ Saving final model to {output}...")
    try:
        output_path = Path("{output}")
        output_path.mkdir(parents=True, exist_ok=True)
        if save_model_and_tokenizer(str(output_path), model, tokenizer):
            print("‚úÖ Fine-tuning completed! Model saved to {output}")
            print("\\nüó®Ô∏è To chat with your model:")
            print("   mlx-toolkit query {output} -i")
            print("\\nüí° For LoRA models, use:")
            print("   python -m mlx_lm.generate --model {model_path} --adapter-path {output} --prompt 'Your question'")
            print("   python -m mlx_lm.chat --model {model_path} --adapter-path {output}  # Interactive chat")
        else:
            print("‚ùå Error saving final model to {output}")
            sys.exit(1)
        
        # Save training info
        info = {{
            "model_path": "{model_path}",
            "data_path": "{data_path}",
            "epochs": {epochs},
            "batch_size": {batch_size},
            "learning_rate": {learning_rate},
            "lora_rank": {lora_rank},
            "lora_alpha": {lora_alpha},
            "total_steps": total_steps,
            "training_samples": len(tokenized_data)
        }}
        
        with open(output_path / "training_info.json", "w") as f:
            json.dump(info, f, indent=2)
        
    except Exception as e:
        print(f"‚ùå Error saving model: {{e}}")
        sys.exit(1)

if __name__ == "__main__":
    main()
"""
    
    # Write and execute fine-tuning script
    script_path = ".mlx_finetune_temp.py"
    with open(script_path, "w") as f:
        f.write(finetune_script)
    
    try:
        console.print("[bold blue]üöÄ Starting fine-tuning process...[/]")
        result = subprocess.run([python_path, script_path], check=True)
        console.print(f"[bold green]‚úÖ Fine-tuning completed successfully![/]")
        console.print(f"[cyan]Model saved to: {output}[/]")
        
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]‚ùå Fine-tuning failed[/]")
        sys.exit(1)
    except KeyboardInterrupt:
        console.print(f"[yellow]‚ö†Ô∏è Fine-tuning interrupted by user[/]")
        sys.exit(1)
    finally:
        # Cleanup
        Path(script_path).unlink(missing_ok=True)


@main.command()
@click.argument("model_path")
@click.option("--prompt", "-p", help="Prompt to test the model")
@click.option("--interactive", "-i", is_flag=True, help="Interactive mode")
@click.option("--max-tokens", default=100, help="Maximum tokens to generate")
@click.option("--temperature", default=0.7, help="Temperature for generation")
def query(model_path, prompt, interactive, max_tokens, temperature):
    """Query a model (auto-handles project environment)"""
    ensure_project_env()
    
    # Use project's Python environment  
    if os.name == 'nt':
        python_path = "venv/Scripts/python"
    else:
        python_path = "venv/bin/python"
    
    if interactive:
        console.print("[bold cyan]Starting interactive chat with your model...[/]")
        console.print("[dim]Type 'quit' to exit[/]")
        
        # Run interactive script in project environment
        script_content = f"""
from mlx_lm import load, generate
import mlx.core as mx

print("Loading model...")
try:
    model, tokenizer = load('{model_path}')
    print("‚úì Model loaded! Type 'quit' to exit.\\n")
except Exception as e:
    print(f"‚ùå Error loading model: {{e}}")
    exit(1)

while True:
    try:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'q']:
            break
        
        if not user_input.strip():
            continue
        
        print("Assistant: ", end="", flush=True)
        
        # Use correct MLX generate API
        response = generate(
            model, 
            tokenizer, 
            prompt=user_input,
            max_tokens={max_tokens},
            verbose=False
        )
        
        # Clean up the response
        if response.startswith(user_input):
            response = response[len(user_input):].strip()
        
        print(response)
        print()  # Add blank line for readability
        
    except KeyboardInterrupt:
        print("\\n\\nGoodbye!")
        break
    except Exception as e:
        print(f"\\n‚ùå Error generating response: {{e}}")
        continue
"""
        
        with open(".mlx_chat_temp.py", "w") as f:
            f.write(script_content)
        
        try:
            subprocess.run([python_path, ".mlx_chat_temp.py"])
        finally:
            Path(".mlx_chat_temp.py").unlink(missing_ok=True)
    
    else:
        if not prompt:
            console.print("[bold red]Please provide a prompt with -p or use -i for interactive mode[/]")
            sys.exit(1)
        
        with console.status("[bold blue]Generating response..."):
            script_content = f"""
from mlx_lm import load, generate

try:
    model, tokenizer = load('{model_path}')
    
    response = generate(
        model, 
        tokenizer, 
        prompt='''{prompt}''', 
        max_tokens={max_tokens},
        verbose=False
    )
    
    # Clean up response if it repeats the prompt
    if response.startswith('''{prompt}'''):
        response = response[len('''{prompt}'''):].strip()
    
    print(response)
    
except Exception as e:
    print(f"Error: {{e}}")
    exit(1)
"""
            
            with open(".mlx_query_temp.py", "w") as f:
                f.write(script_content)
            
            try:
                result = subprocess.run([python_path, ".mlx_query_temp.py"], 
                                      capture_output=True, text=True, check=True)
                response_text = result.stdout.strip()
                if response_text:
                    console.print(Panel(response_text, title="Response", border_style="blue"))
                else:
                    console.print("[yellow]No response generated[/]")
            except subprocess.CalledProcessError as e:
                console.print(f"[bold red]Failed to generate response[/]")
                if result.stderr:
                    console.print(f"[red]Error: {result.stderr}[/]")
            finally:
                Path(".mlx_query_temp.py").unlink(missing_ok=True)


@main.command()
@click.argument("model_path")
@click.argument("test_data")
@click.option("--metrics", "-m", multiple=True, default=["perplexity"], 
              help="Metrics to evaluate (perplexity, generation_quality)")
@click.option("--num-samples", default=50, help="Number of samples to evaluate")
@click.option("--max-tokens", default=100, help="Max tokens for generation tasks")
@click.option("--output", "-o", help="Output file for detailed results")
def test(model_path, test_data, metrics, num_samples, max_tokens, output):
    """Test and evaluate a model with comprehensive metrics"""
    ensure_project_env()
    
    # Validate inputs
    if not Path(model_path).exists():
        console.print(f"[red]‚ùå Model not found: {model_path}[/]")
        sys.exit(1)
    
    if not Path(test_data).exists():
        console.print(f"[red]‚ùå Test data not found: {test_data}[/]")
        sys.exit(1)
    
    console.print(Panel(
        f"[bold blue]Model Evaluation Setup:[/]\n"
        f"Model: {model_path}\n"
        f"Test Data: {test_data}\n"
        f"Metrics: {', '.join(metrics)}\n"
        f"Samples: {num_samples}",
        title="Evaluation Configuration",
        border_style="blue"
    ))
    
    if os.name == 'nt':
        python_path = "venv/Scripts/python"
    else:
        python_path = "venv/bin/python"
    
    # Create evaluation script
    eval_script = f"""
import sys
import json
import time
import math
from pathlib import Path
from typing import Dict, List, Any

import mlx.core as mx
from mlx_lm import load, generate

def load_test_data(data_path: str, max_samples: int = {num_samples}):
    \"\"\"Load test data from JSONL file\"\"\"
    data = []
    try:
        with open(data_path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                if len(data) >= max_samples:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    item = json.loads(line)
                    data.append(item)
                except json.JSONDecodeError as e:
                    print(f"Warning: Error parsing line {{line_num}}: {{e}}")
                    continue
        
        print(f"Loaded {len(data)} test samples")
        return data
        
    except Exception as e:
        print(f"Error loading test data: {{e}}")
        sys.exit(1)

def calculate_perplexity(model, tokenizer, data):
    \"\"\"Calculate perplexity on test data\"\"\"
    print("Calculating perplexity...")
    
    total_log_prob = 0
    total_tokens = 0
    
    for i, item in enumerate(data):
        if i % 10 == 0:
            print(f"  Processing {i}/{len(data)}...")
        
        # Get text to evaluate
        if 'text' in item:
            text = item['text']
        elif 'instruction' in item and 'response' in item:
            text = f"### Instruction: {{item['instruction']}}\\n### Response: {{item['response']}}"
        else:
            continue
        
        try:
            # Tokenize using encode method (compatible with TokenizerWrapper)
            tokens = tokenizer.encode(text)
            if len(tokens) < 2:
                continue
            
            # Convert to MLX array
            token_array = mx.array(tokens)
            
            # Get model logits
            logits = model(token_array[:-1][None])  # Exclude last token for input
            
            # Calculate log probabilities
            log_probs = mx.log_softmax(logits[0], axis=-1)
            
            # Get log probability of actual next tokens
            target_tokens = token_array[1:]  # Exclude first token for targets
            target_log_probs = log_probs[mx.arange(len(target_tokens)), target_tokens]
            
            # Sum log probabilities
            total_log_prob += mx.sum(target_log_probs).item()
            total_tokens += len(target_tokens)
            
        except Exception as e:
            print(f"Warning: Error processing item {{i}}: {{e}}")
            continue
    
    if total_tokens > 0:
        perplexity = math.exp(-total_log_prob / total_tokens)
        return perplexity
    else:
        return float('inf')

def evaluate_generation_quality(model, tokenizer, data, max_tokens={max_tokens}):
    \"\"\"Evaluate generation quality with sample outputs\"\"\"
    print("Evaluating generation quality...")
    
    samples = []
    response_lengths = []
    generation_times = []
    
    # Select subset for generation evaluation
    eval_data = data[:min(10, len(data))]
    
    for i, item in enumerate(eval_data):
        print(f"  Generating {i+1}/{len(eval_data)}...")
        
        # Create prompt
        if 'instruction' in item:
            prompt = f"### Instruction: {{item['instruction']}}\\n### Response:"
            expected = item.get('response', 'N/A')
        elif 'text' in item:
            # Use first part as prompt, rest as expected
            words = item['text'].split()
            if len(words) > 10:
                prompt = ' '.join(words[:len(words)//2])
                expected = ' '.join(words[len(words)//2:])
            else:
                prompt = item['text']
                expected = 'N/A'
        else:
            continue
        
        try:
            # Time the generation
            start_time = time.time()
            
            response = generate(
                model, 
                tokenizer, 
                prompt=prompt,
                max_tokens=max_tokens,
                verbose=False
            )
            
            end_time = time.time()
            
            # Clean up response
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            generation_time = end_time - start_time
            response_length = len(tokenizer.encode(response))
            
            samples.append({{
                'prompt': prompt,
                'expected': expected,
                'generated': response,
                'generation_time': generation_time,
                'response_length': response_length
            }})
            
            response_lengths.append(response_length)
            generation_times.append(generation_time)
            
        except Exception as e:
            print(f"Warning: Error generating for item {{i}}: {{e}}")
            continue
    
    # Calculate statistics
    if response_lengths:
        avg_length = sum(response_lengths) / len(response_lengths)
        avg_time = sum(generation_times) / len(generation_times)
        tokens_per_sec = avg_length / avg_time if avg_time > 0 else 0
    else:
        avg_length = avg_time = tokens_per_sec = 0
    
    return {{
        'samples': samples,
        'avg_response_length': avg_length,
        'avg_generation_time': avg_time,
        'tokens_per_second': tokens_per_sec,
        'total_samples': len(samples)
    }}

def main():
    print("üß™ Starting Model Evaluation...")
    
    # Load model
    print(f"Loading model from {{'{model_path}'}}...")
    try:
        model, tokenizer = load('{model_path}')
        print("‚úì Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Error loading model: {{e}}")
        sys.exit(1)
    
    # Load test data
    test_data = load_test_data('{test_data}')
    
    if not test_data:
        print("‚ùå No valid test data found")
        sys.exit(1)
    
    # Run evaluations
    results = {{}}
    
    metrics = {{list(metrics)}}
    
    if 'perplexity' in metrics:
        try:
            ppl = calculate_perplexity(model, tokenizer, test_data)
            results['perplexity'] = ppl
            print(f"‚úì Perplexity: {ppl:.2f}")
        except Exception as e:
            print(f"‚ùå Error calculating perplexity: {{e}}")
            results['perplexity'] = 'Error'
    
    if 'generation_quality' in metrics:
        try:
            gen_results = evaluate_generation_quality(model, tokenizer, test_data)
            results['generation_quality'] = gen_results
            print(f"‚úì Generation Quality: {gen_results['total_samples']} samples")
            print(f"  Avg Response Length: {gen_results['avg_response_length']:.1f} tokens")
            print(f"  Tokens/Second: {gen_results['tokens_per_second']:.1f}")
        except Exception as e:
            print(f"‚ùå Error evaluating generation: {{e}}")
            results['generation_quality'] = 'Error'
    
    # Save detailed results if requested
    if '{output}':
        try:
            with open('{output}', 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"üìÑ Detailed results saved to {{'{output}'}}")
        except Exception as e:
            print(f"Warning: Could not save results: {{e}}")
    
    # Print summary
    print("\\n" + "="*50)
    print("EVALUATION SUMMARY")
    print("="*50)
    
    for metric, value in results.items():
        if metric == 'generation_quality' and isinstance(value, dict):
            print(f"Generation Quality:")
            print(f"  Samples: {value.get('total_samples', 0)}")
            print(f"  Avg Length: {value.get('avg_response_length', 0):.1f} tokens")
            print(f"  Speed: {value.get('tokens_per_second', 0):.1f} tokens/sec")
        elif isinstance(value, float):
            print(f"{metric.title()}: {value:.4f}")
        else:
            print(f"{metric.title()}: {value}")
    
    print("="*50)
    print("‚úÖ Evaluation completed!")

if __name__ == "__main__":
    main()
"""
    
    # Write and execute evaluation script
    script_path = ".mlx_eval_temp.py"
    with open(script_path, "w") as f:
        f.write(eval_script)
    
    try:
        console.print("[bold blue]üß™ Starting model evaluation...[/]")
        result = subprocess.run([python_path, script_path], check=True)
        console.print(f"[bold green]‚úÖ Evaluation completed![/]")
        
        if output:
            console.print(f"[cyan]Detailed results saved to: {output}[/]")
        
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]‚ùå Evaluation failed[/]")
        sys.exit(1)
    except KeyboardInterrupt:
        console.print(f"[yellow]‚ö†Ô∏è Evaluation interrupted by user[/]")
        sys.exit(1)
    finally:
        # Cleanup
        Path(script_path).unlink(missing_ok=True)


@main.command()
def list_templates():
    """List available project templates"""
    templates = {
        "basic": "Simple MLX project with minimal structure",
        "advanced": "Full-featured project with training pipelines and utilities"
    }
    
    console.print(Panel(
        "\n".join([f"[bold]{name}[/]: {desc}" for name, desc in templates.items()]),
        title="Available Templates",
        border_style="blue"
    ))


@main.command()
@click.option("--format", type=click.Choice(["instruction", "text", "both"]), default="both")
@click.option("--samples", default=50, help="Number of sample entries to create")
def create_training_data(format, samples):
    """Create training data templates and examples"""
    ensure_project_env()
    
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    
    console.print(Panel(
        f"[bold green]Creating Training Data Templates[/]\n"
        f"Format: {format}\n"
        f"Samples: {samples}\n"
        f"Output: data/ directory",
        title="Data Creation",
        border_style="green"
    ))
    
    if format in ["instruction", "both"]:
        create_instruction_data(data_dir, samples)
    
    if format in ["text", "both"]:
        create_text_data(data_dir, samples)
    
    create_data_documentation(data_dir)
    
    console.print(f"[bold green]‚úÖ Training data templates created in data/ directory![/]")
    console.print(f"[cyan]üìö See data/README.md for detailed instructions[/]")


def create_instruction_data(data_dir: Path, samples: int):
    """Create instruction-response training data template"""
    
    # Create training data (80% of samples)
    train_file = data_dir / "train.jsonl"
    train_samples = int(samples * 0.8)
    
    with open(train_file, "w") as f:
        instructions = [
            ("Explain what machine learning is", "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed for every task."),
            ("Write a Python function to calculate factorial", "def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)"),
            ("What are the benefits of renewable energy?", "Renewable energy offers several benefits including reduced greenhouse gas emissions, energy independence, job creation, and long-term cost savings."),
            ("How do you make a good first impression?", "To make a good first impression: be punctual, dress appropriately, maintain eye contact, offer a firm handshake, listen actively, and show genuine interest in others."),
            ("Explain the water cycle", "The water cycle consists of evaporation (water turns to vapor), condensation (vapor forms clouds), precipitation (rain/snow falls), and collection (water gathers in bodies of water)."),
            ("What is photosynthesis?", "Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize nutrients from carbon dioxide and water, generating oxygen as a byproduct."),
            ("How do computers work?", "Computers work by processing binary data (0s and 1s) through electronic circuits, executing instructions stored in memory to perform calculations and operations."),
            ("What is the purpose of a business plan?", "A business plan serves as a roadmap for your business, outlining goals, strategies, target market, financial projections, and operational plans to guide decision-making and attract investors."),
        ]
        
        for i in range(train_samples):
            instruction, response = instructions[i % len(instructions)]
            if i >= len(instructions):
                # Add variation to prevent overfitting
                instruction = f"[Training Example {i+1}] {instruction}"
                response = f"[Training Response {i+1}] {response}"
            
            entry = {
                "instruction": instruction,
                "response": response
            }
            f.write(json.dumps(entry) + "\\n")
    
    # Create validation data (20% of samples, minimum 5 examples)
    val_file = data_dir / "validation.jsonl"
    val_samples = max(5, int(samples * 0.2))
    
    with open(val_file, "w") as f:
        val_instructions = [
            ("Define artificial intelligence", "Artificial intelligence is the simulation of human intelligence in machines that are programmed to think and learn like humans."),
            ("Write a simple sorting algorithm", "def bubble_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n    return arr"),
            ("Explain climate change", "Climate change refers to long-term shifts in global temperatures and weather patterns, primarily driven by human activities like burning fossil fuels, which increase greenhouse gas concentrations."),
            ("What is democracy?", "Democracy is a system of government where power is held by the people, either directly or through elected representatives, emphasizing equality, freedom, and majority rule with minority rights."),
            ("How does the internet work?", "The internet works through a network of interconnected computers that communicate using standardized protocols, routing data through various servers and infrastructure to connect users worldwide."),
        ]
        
        for i in range(val_samples):
            instruction, response = val_instructions[i % len(val_instructions)]
            if i >= len(val_instructions):
                instruction = f"[Validation Example {i+1}] {instruction}"
                response = f"[Validation Response {i+1}] {response}"
            
            entry = {
                "instruction": instruction,
                "response": response
            }
            f.write(json.dumps(entry) + "\\n")
    
    console.print(f"‚úì Created training data: {train_file} ({train_samples} examples)")
    console.print(f"‚úì Created validation data: {val_file} ({val_samples} examples)")
    console.print(f"[cyan]üìä Split: 80% training, 20% validation (industry standard)[/]")


def create_text_data(data_dir: Path, samples: int):
    """Create text-only training data template"""
    
    # Create training data
    train_file = data_dir / "train_text.jsonl"
    with open(train_file, "w") as f:
        texts = [
            "The quick brown fox jumps over the lazy dog. This pangram contains every letter of the English alphabet at least once.",
            "Climate change refers to long-term shifts in global temperatures and weather patterns. While climate variations are natural, human activities have been the main driver since the 1800s.",
            "Python is a high-level programming language known for its simplicity and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.",
            "The Internet is a global network of interconnected computers that communicate using standardized protocols. It has revolutionized how we access information and communicate.",
            "Photosynthesis is the process by which plants convert light energy into chemical energy. This process is fundamental to most life on Earth as it produces oxygen and organic compounds.",
        ]
        
        for i in range(samples):
            text = texts[i % len(texts)]
            if i >= len(texts):
                text = f"[Example {i+1}] {text}"
            
            entry = {{"text": text}}
            f.write(json.dumps(entry) + "\\n")
    
    # Create validation data
    val_file = data_dir / "val_text.jsonl"
    with open(val_file, "w") as f:
        val_texts = [
            "Machine learning algorithms can identify patterns in data that humans might miss. This capability makes them valuable for tasks like image recognition and natural language processing.",
            "The scientific method involves making observations, forming hypotheses, conducting experiments, and drawing conclusions. This systematic approach has driven scientific progress for centuries.",
        ]
        
        for text in val_texts:
            entry = {{"text": text}}
            f.write(json.dumps(entry) + "\\n")
    
    console.print(f"‚úì Created text-only data: {train_file} ({samples} samples)")
    console.print(f"‚úì Created validation data: {val_file} (2 samples)")


def create_data_documentation(data_dir: Path):
    """Create comprehensive data documentation"""
    
    readme_content = f"""# üöÄ MLX Fine-tuning Data Guide

**Professional training data preparation for Llama models and MLX fine-tuning on Apple Silicon**

## üìÅ Project Structure

Your MLX project should be organized like this:

```
my-ai-project/
‚îú‚îÄ‚îÄ data/                     # üìä ALL TRAINING DATA GOES HERE
‚îÇ   ‚îú‚îÄ‚îÄ README.md            # This comprehensive guide  
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl          # üéØ Your main training data
‚îÇ   ‚îú‚îÄ‚îÄ validation.jsonl     # ‚úÖ Validation set for monitoring
‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl          # üß™ Final evaluation data
‚îÇ   ‚îî‚îÄ‚îÄ raw/                # üì• Original data before processing
‚îú‚îÄ‚îÄ models/                  # ü§ñ Downloaded and fine-tuned models
‚îÇ   ‚îî‚îÄ‚îÄ Llama-3.1-8B-Instruct-4bit/
‚îú‚îÄ‚îÄ fine_tuned_models/      # üíé Your custom trained models
‚îÇ   ‚îî‚îÄ‚îÄ my_custom_llama/
‚îî‚îÄ‚îÄ scripts/                # üîß Custom preprocessing scripts
```

## üéØ Llama-Optimized Data Formats

### Format 1: Llama Chat Template (RECOMMENDED)

**Perfect for Llama-3.1-8B-Instruct and chat models**

```jsonl
{{"instruction": "What is machine learning?", "response": "Machine learning is a method of data analysis that automates analytical model building. It's a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention."}}
{{"instruction": "Write a Python function to calculate fibonacci numbers", "response": "def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\\n\\n# More efficient iterative version:\\ndef fibonacci_iter(n):\\n    if n <= 1:\\n        return n\\n    a, b = 0, 1\\n    for _ in range(2, n + 1):\\n        a, b = b, a + b\\n    return b"}}
{{"instruction": "Explain the concept of recursion in programming", "response": "Recursion is a programming technique where a function calls itself to solve a problem. It breaks down complex problems into smaller, similar subproblems. Every recursive function needs two key components: a base case (stopping condition) and a recursive case (where the function calls itself with modified parameters)."}}
```

**‚ú® Why this format works best for Llama:**
- Matches Llama's instruction-following training
- Clear separation between user intent and model response
- Optimal for chat, Q&A, and task-specific applications
- Compatible with Llama's attention patterns

### Format 2: Conversation Multi-turn

**For training conversational AI with context**

```jsonl
{{"instruction": "I'm planning a trip to Japan. What should I know?", "response": "Japan is an amazing destination! Here are key things to know:\\n\\nüóæ **Best Times to Visit:**\\n- Spring (March-May): Cherry blossoms, mild weather\\n- Fall (September-November): Beautiful foliage, comfortable temperatures\\n\\nüé´ **Essential Preparations:**\\n- Visa requirements (depends on your country)\\n- JR Pass for train travel\\n- Cash-based society (bring yen)\\n- Pocket WiFi or international phone plan\\n\\nüçú **Cultural Tips:**\\n- Bow when greeting\\n- Remove shoes when entering homes\\n- Slurping noodles is polite\\n- Tipping is not customary\\n\\nWhat specific aspects of Japan interest you most?"}}
{{"instruction": "I'm most interested in the food scene and traditional culture. Can you elaborate?", "response": "Excellent choices! Japan's culinary and cultural heritage are extraordinary:\\n\\nüç£ **Food Scene:**\\n- **Sushi**: Try authentic sushi at Tsukiji Outer Market\\n- **Ramen**: Regional varieties - Tonkotsu (Kyushu), Miso (Hokkaido), Shoyu (Tokyo)\\n- **Kaiseki**: Multi-course traditional dining experience\\n- **Street Food**: Takoyaki in Osaka, Taiyaki, Yakitori\\n- **Izakayas**: Traditional gastropubs for authentic experience\\n\\nüèÆ **Traditional Culture:**\\n- **Temples & Shrines**: Fushimi Inari (Kyoto), Senso-ji (Tokyo)\\n- **Tea Ceremony**: Book a traditional chanoyu experience\\n- **Onsen**: Natural hot springs (follow etiquette strictly)\\n- **Ryokan**: Traditional inns with tatami mats and futon beds\\n- **Festivals**: Check local matsuri during your visit\\n\\n**Pro tip**: Stay in Kyoto's Gion district to see geishas and traditional architecture!"}}
```

### Format 3: Code-Focused Training

**Optimized for programming assistance and technical tasks**

```jsonl
{{"instruction": "Debug this Python code that's supposed to find the second largest number in a list", "response": "I'd be happy to help debug your code! However, I don't see the code in your message. Could you please share the code you're trying to debug?\\n\\nIn the meantime, here's a correct implementation for finding the second largest number:\\n\\n```python\\ndef find_second_largest(numbers):\\n    if len(numbers) < 2:\\n        return None  # Need at least 2 numbers\\n    \\n    # Remove duplicates and sort\\n    unique_numbers = list(set(numbers))\\n    \\n    if len(unique_numbers) < 2:\\n        return None  # All numbers are the same\\n    \\n    unique_numbers.sort(reverse=True)\\n    return unique_numbers[1]\\n\\n# Alternative O(n) solution:\\ndef find_second_largest_optimized(numbers):\\n    if len(numbers) < 2:\\n        return None\\n        \\n    first = second = float('-inf')\\n    \\n    for num in numbers:\\n        if num > first:\\n            second = first\\n            first = num\\n        elif num > second and num != first:\\n            second = num\\n    \\n    return second if second != float('-inf') else None\\n```\\n\\nCommon bugs in this type of code:\\n- Not handling lists with fewer than 2 elements\\n- Not handling duplicate values\\n- Off-by-one errors in indexing\\n\\nPlease share your code and I'll help identify the specific issue!"}}
```

## üìè Data Specifications for Apple Silicon

### For M4 Pro (24GB RAM) - OPTIMAL SETTINGS

```bash
# Recommended fine-tuning parameters
mlx-toolkit finetune models/Llama-3.1-8B-Instruct-4bit data/train.jsonl \\
  --output fine_tuned_models/my_llama \\
  --epochs 3 \\
  --batch-size 6 \\
  --learning-rate 1e-5 \\
  --lora-rank 16 \\
  --lora-alpha 32 \\
  --max-seq-length 2048
```

### Data Size Guidelines

| **Purpose** | **Training Examples** | **Validation Examples** | **Expected Results** |
|-------------|----------------------|-------------------------|---------------------|
| **Quick Adaptation** | 50-200 | 10-20 | Basic task learning |
| **Domain Specialization** | 500-1,500 | 50-150 | Strong domain knowledge |
| **Professional Model** | 2,000-10,000 | 200-500 | Production-ready |
| **Research/Competition** | 10,000+ | 1,000+ | State-of-the-art |

### Token Length Optimization

- **Optimal length**: 512-1024 tokens per example
- **Maximum**: 2048 tokens (hardware limit)
- **Minimum**: 50 tokens (effective learning)

**üí° Pro tip**: Llama models perform best with examples that are substantial but not overwhelming. Aim for complete, well-structured responses rather than very long ones.

## üõ†Ô∏è Data Preparation Workflow

### Step 1: Data Collection & Organization

```bash
# Create proper directory structure
mkdir -p data/raw data/processed

# Store your original data
cp your_raw_data.csv data/raw/
```

### Step 2: Convert to MLX Format

**Python script for conversion:**

```python
#!/usr/bin/env python3
\"\"\"Convert various formats to MLX training data\"\"\"

import json
import csv
import pandas as pd
from pathlib import Path

def csv_to_training_data(csv_file, output_file):
    \\\"\\\"\\\"Convert CSV with 'question' and 'answer' columns to training format\\\"\\\"\\\"
    
    with open(csv_file, 'r') as infile, open(output_file, 'w') as outfile:
        reader = csv.DictReader(infile)
        
        for row in reader:
            # Adapt these column names to your CSV
            instruction = row.get('question', row.get('prompt', row.get('input', '')))
            response = row.get('answer', row.get('response', row.get('output', '')))
            
            if instruction and response:
                training_example = {{
                    "instruction": instruction.strip(),
                    "response": response.strip()
                }}
                outfile.write(json.dumps(training_example) + '\\n')

def text_to_training_data(text_file, output_file):
    \\\"\\\"\\\"Convert plain text to instruction-response format\\\"\\\"\\\"
    
    with open(text_file, 'r') as infile, open(output_file, 'w') as outfile:
        content = infile.read()
        
        # Split into paragraphs
        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]
        
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph) > 100:  # Only use substantial paragraphs
                training_example = {{
                    "instruction": f"Continue this text passage:",
                    "response": paragraph
                }}
                outfile.write(json.dumps(training_example) + '\\n')

# Usage examples
if __name__ == "__main__":
    # Convert your data
    csv_to_training_data('data/raw/qa_pairs.csv', 'data/train.jsonl')
    
    print("‚úÖ Data conversion complete!")
```

### Step 3: Data Validation & Quality Check

```python
#!/usr/bin/env python3
\"\"\"Validate training data quality\"\"\"

import json
import statistics
from collections import Counter

def analyze_training_data(jsonl_file):
    \\\"\\\"\\\"Comprehensive analysis of training data\\\"\\\"\\\"
    
    examples = []
    instruction_lengths = []
    response_lengths = []
    
    print(f"üìä Analyzing {jsonl_file}...")
    
    with open(jsonl_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())
                
                if 'instruction' in data and 'response' in data:
                    examples.append(data)
                    instruction_lengths.append(len(data['instruction']))
                    response_lengths.append(len(data['response']))
                else:
                    print(f"‚ö†Ô∏è  Line {line_num}: Missing required fields")
                    
            except json.JSONDecodeError as e:
                print(f"‚ùå Line {line_num}: Invalid JSON - {e}")
    
    if examples:
        print(f"\\n‚úÖ Found {len(examples)} valid examples")
        print(f"üìè Instruction length: {statistics.mean(instruction_lengths):.1f} ¬± {statistics.stdev(instruction_lengths):.1f} chars")
        print(f"üìè Response length: {statistics.mean(response_lengths):.1f} ¬± {statistics.stdev(response_lengths):.1f} chars")
        
        # Check for duplicates
        instructions = [ex['instruction'] for ex in examples]
        duplicates = len(instructions) - len(set(instructions))
        if duplicates > 0:
            print(f"‚ö†Ô∏è  Found {duplicates} duplicate instructions")
        
        # Show example
        print(f"\\nüìù Sample example:")
        sample = examples[0]
        print(f"Instruction: {sample['instruction'][:100]}...")
        print(f"Response: {sample['response'][:100]}...")
        
        return True
    else:
        print("‚ùå No valid examples found!")
        return False

# Run analysis
if __name__ == "__main__":
    analyze_training_data('data/train.jsonl')
    analyze_training_data('data/validation.jsonl')
```

### Step 4: Create Train/Validation Split

```python
#!/usr/bin/env python3
\"\"\"Split data into training and validation sets\"\"\"

import json
import random
from pathlib import Path

def create_train_val_split(input_file, train_ratio=0.8):
    \\\"\\\"\\\"Split data into training and validation sets\\\"\\\"\\\"
    
    # Load all examples
    examples = []
    with open(input_file, 'r') as f:
        for line in f:
            if line.strip():
                examples.append(json.loads(line.strip()))
    
    # Shuffle and split
    random.shuffle(examples)
    split_point = int(len(examples) * train_ratio)
    
    train_examples = examples[:split_point]
    val_examples = examples[split_point:]
    
    # Write training set
    with open('data/train.jsonl', 'w') as f:
        for example in train_examples:
            f.write(json.dumps(example) + '\\n')
    
    # Write validation set
    with open('data/validation.jsonl', 'w') as f:
        for example in val_examples:
            f.write(json.dumps(example) + '\\n')
    
    print(f"‚úÖ Created {len(train_examples)} training examples")
    print(f"‚úÖ Created {len(val_examples)} validation examples")

# Usage
if __name__ == "__main__":
    create_train_val_split('data/all_data.jsonl', train_ratio=0.85)
```

## üéØ Llama-Specific Best Practices

### 1. Instruction Design for Llama

**‚úÖ GOOD Examples:**

```jsonl
{{"instruction": "Explain the water cycle in simple terms", "response": "The water cycle is nature's way of recycling water through four main steps:\\n\\n1. **Evaporation**: Heat from the sun turns water in oceans, lakes, and rivers into invisible water vapor that rises into the air\\n\\n2. **Condensation**: As water vapor rises higher, it cools down and turns back into tiny water droplets, forming clouds\\n\\n3. **Precipitation**: When clouds become heavy with water droplets, they release the water as rain, snow, sleet, or hail\\n\\n4. **Collection**: The fallen water flows into rivers, lakes, and oceans, or soaks into the ground, completing the cycle\\n\\nThis process repeats continuously, ensuring Earth always has fresh water available for all living things!"}}
```

**‚ùå Problematic Examples:**

```jsonl
{{"instruction": "water cycle", "response": "evaporation condensation precipitation"}}
{{"instruction": "What is the water cycle? How does it work? Why is it important? What are the steps? Can you explain each step in detail?", "response": "Yes."}}
```

### 2. Response Quality Standards

**Llama models excel when trained on responses that are:**

- **Comprehensive but focused**: Cover the topic thoroughly without rambling
- **Well-structured**: Use bullet points, numbered lists, and clear sections
- **Conversational**: Natural tone that feels human
- **Accurate**: Factually correct information
- **Actionable**: Include practical advice when relevant

### 3. Domain-Specific Training

**For specialized domains, create focused datasets:**

```jsonl
// Medical Assistant Example
{{"instruction": "What are the common symptoms of dehydration?", "response": "Common symptoms of dehydration include:\\n\\n**Mild Dehydration:**\\n‚Ä¢ Thirst\\n‚Ä¢ Dry mouth and lips\\n‚Ä¢ Dark yellow urine\\n‚Ä¢ Fatigue\\n‚Ä¢ Dizziness\\n\\n**Moderate Dehydration:**\\n‚Ä¢ Very dark urine or no urination\\n‚Ä¢ Dry skin that doesn't bounce back when pinched\\n‚Ä¢ Rapid heartbeat\\n‚Ä¢ Headache\\n‚Ä¢ Confusion\\n\\n**Severe Dehydration (Medical Emergency):**\\n‚Ä¢ Extremely dry mouth and mucous membranes\\n‚Ä¢ Lack of sweating\\n‚Ä¢ Sunken eyes\\n‚Ä¢ Rapid, weak pulse\\n‚Ä¢ Loss of consciousness\\n\\n**Important**: If you suspect severe dehydration, seek immediate medical attention. For mild dehydration, gradually increase fluid intake with water or electrolyte solutions."}}

// Code Assistant Example  
{{"instruction": "How do I handle exceptions in Python when reading a file?", "response": "Here are the best practices for handling file reading exceptions in Python:\\n\\n**Basic Exception Handling:**\\n```python\\ntry:\\n    with open('filename.txt', 'r') as file:\\n        content = file.read()\\n        print(content)\\nexcept FileNotFoundError:\\n    print('Error: File not found')\\nexcept PermissionError:\\n    print('Error: Permission denied')\\nexcept Exception as e:\\n    print(f'Unexpected error: {{e}}')\\n```\\n\\n**More Robust Approach:**\\n```python\\nimport os\\nfrom pathlib import Path\\n\\ndef read_file_safely(filepath):\\n    try:\\n        # Check if file exists first\\n        if not Path(filepath).exists():\\n            raise FileNotFoundError(f'File {{filepath}} does not exist')\\n        \\n        with open(filepath, 'r', encoding='utf-8') as file:\\n            return file.read()\\n            \\n    except FileNotFoundError as e:\\n        print(f'File error: {{e}}')\\n        return None\\n    except PermissionError:\\n        print(f'Permission denied: Cannot read {{filepath}}')\\n        return None\\n    except UnicodeDecodeError:\\n        print(f'Encoding error: Cannot decode {{filepath}} as UTF-8')\\n        return None\\n    except Exception as e:\\n        print(f'Unexpected error reading {{filepath}}: {{e}}')\\n        return None\\n```\\n\\n**Key Points:**\\n‚Ä¢ Always use `with open()` for automatic file closing\\n‚Ä¢ Handle specific exceptions before general ones\\n‚Ä¢ Provide meaningful error messages\\n‚Ä¢ Consider encoding issues with text files\\n‚Ä¢ Return None or default values for error cases"}}
```

## üß™ Testing Your Training Data

### Quick Quality Check Script

```bash
# Create this as data/validate.py
#!/usr/bin/env python3

import json
import sys

def quick_validation(jsonl_file):
    print(f"üîç Validating {jsonl_file}...")
    
    valid = invalid = 0
    
    with open(jsonl_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())
                if 'instruction' in data and 'response' in data:
                    if len(data['instruction']) > 10 and len(data['response']) > 20:
                        valid += 1
                    else:
                        print(f"Line {line_num}: Too short")
                        invalid += 1
                else:
                    print(f"Line {line_num}: Missing fields")
                    invalid += 1
            except:
                print(f"Line {line_num}: Invalid JSON")
                invalid += 1
    
    print(f"‚úÖ Valid: {valid}")
    print(f"‚ùå Invalid: {invalid}")
    print(f"üìä Success rate: {valid/(valid+invalid)*100:.1f}%")
    
    return valid > 0 and (valid/(valid+invalid)) > 0.9

if __name__ == "__main__":
    if len(sys.argv) > 1:
        quick_validation(sys.argv[1])
    else:
        quick_validation('train.jsonl')
```

**Run validation:**
```bash
cd data
python validate.py train.jsonl
python validate.py validation.jsonl
```

## üöÄ Complete Fine-tuning Workflow

### 1. Data Preparation
```bash
# Create training data templates
mlx-toolkit create-training-data --format instruction --samples 100

# Edit your data
nano data/train_instruction.jsonl

# Validate data quality
python data/validate.py data/train_instruction.jsonl
```

### 2. Model Fine-tuning
```bash
# Start fine-tuning with optimal settings for M4 Pro
mlx-toolkit finetune models/Llama-3.1-8B-Instruct-4bit data/train_instruction.jsonl \\
  --output fine_tuned_models/my_specialized_llama \\
  --epochs 3 \\
  --batch-size 6 \\
  --learning-rate 1e-5 \\
  --lora-rank 16 \\
  --max-seq-length 1024 \\
  --save-every 50
```

### 3. Model Evaluation
```bash
# Test your fine-tuned model
mlx-toolkit test fine_tuned_models/my_specialized_llama data/validation.jsonl \\
  --metrics perplexity generation_quality \\
  --num-samples 50 \\
  --output evaluation_results.json

# Interactive testing
mlx-toolkit query fine_tuned_models/my_specialized_llama -i
```

## üéØ Advanced Techniques

### Multi-turn Conversation Training

```jsonl
{{"instruction": "I'm learning Python and struggling with object-oriented programming concepts", "response": "Object-oriented programming (OOP) can definitely feel overwhelming at first! Let me break it down into digestible concepts:\\n\\n**Core OOP Concepts:**\\n\\n1. **Classes & Objects**\\n   - Class = Blueprint (like a house plan)\\n   - Object = Instance (like an actual house built from the plan)\\n\\n2. **Key Principles:**\\n   - **Encapsulation**: Bundling data and methods together\\n   - **Inheritance**: Creating new classes based on existing ones\\n   - **Polymorphism**: Same method name, different behaviors\\n\\n**Simple Example:**\\n```python\\nclass Dog:\\n    def __init__(self, name, breed):\\n        self.name = name\\n        self.breed = breed\\n    \\n    def bark(self):\\n        return f'{{self.name}} says Woof!'\\n\\n# Create objects\\nbuddy = Dog('Buddy', 'Golden Retriever')\\nmax_dog = Dog('Max', 'German Shepherd')\\n\\nprint(buddy.bark())  # Buddy says Woof!\\n```\\n\\nWhat specific part of OOP is giving you the most trouble? Classes, inheritance, or something else?"}}
{{"instruction": "I understand classes and objects now, but inheritance is confusing me. Can you explain it with a practical example?", "response": "Excellent! Inheritance is actually one of the most powerful features of OOP. Think of it like family traits passed down from parents to children.\\n\\n**Inheritance Basics:**\\n- **Parent Class (Base/Super)**: The original class\\n- **Child Class (Derived/Sub)**: Inherits from parent\\n- Child gets all parent methods + can add its own\\n- Child can override parent methods\\n\\n**Practical Example - Vehicle System:**\\n\\n```python\\n# Parent class\\nclass Vehicle:\\n    def __init__(self, brand, model, year):\\n        self.brand = brand\\n        self.model = model\\n        self.year = year\\n        self.is_running = False\\n    \\n    def start(self):\\n        self.is_running = True\\n        return f'{{self.brand}} {{self.model}} started!'\\n    \\n    def stop(self):\\n        self.is_running = False\\n        return f'{{self.brand}} {{self.model}} stopped!'\\n\\n# Child class - inherits from Vehicle\\nclass Car(Vehicle):\\n    def __init__(self, brand, model, year, doors):\\n        super().__init__(brand, model, year)  # Call parent constructor\\n        self.doors = doors  # Car-specific attribute\\n    \\n    def honk(self):  # Car-specific method\\n        return 'Beep beep!'\\n    \\n    def start(self):  # Override parent method\\n        result = super().start()  # Call parent start()\\n        return result + ' Ready to drive!'\\n\\n# Another child class\\nclass Motorcycle(Vehicle):\\n    def __init__(self, brand, model, year, cc):\\n        super().__init__(brand, model, year)\\n        self.cc = cc\\n    \\n    def wheelie(self):\\n        return 'Doing a wheelie!'\\n    \\n    def start(self):\\n        result = super().start()\\n        return result + ' Vroom vroom!'\\n\\n# Usage\\nmy_car = Car('Toyota', 'Camry', 2023, 4)\\nmy_bike = Motorcycle('Honda', 'CBR', 2022, 600)\\n\\nprint(my_car.start())    # Toyota Camry started! Ready to drive!\\nprint(my_car.honk())     # Beep beep!\\nprint(my_bike.start())   # Honda CBR started! Vroom vroom!\\nprint(my_bike.wheelie()) # Doing a wheelie!\\n```\\n\\n**Key Benefits:**\\n‚úÖ **Code Reuse**: Don't repeat vehicle logic\\n‚úÖ **Extensibility**: Easy to add new vehicle types\\n‚úÖ **Polymorphism**: All vehicles can start(), but differently\\n‚úÖ **Maintenance**: Change vehicle logic once, affects all children\\n\\nDoes this make inheritance clearer? Would you like to see how to use multiple inheritance or abstract classes?"}}
```

### Domain Adaptation Examples

**Customer Service Bot:**
```jsonl
{{"instruction": "A customer is angry that their order hasn't arrived yet", "response": "I completely understand your frustration, and I sincerely apologize for the delay with your order. Let me help resolve this immediately.\\n\\n**Here's what I'll do right now:**\\n\\n1. **Locate your order**: Could you please provide your order number or the email address used for the purchase?\\n\\n2. **Check shipping status**: I'll track exactly where your package is and what caused the delay\\n\\n3. **Provide solutions**: Depending on the situation, I can:\\n   - Expedite shipping at no cost\\n   - Issue a partial refund for the inconvenience\\n   - Arrange a replacement order\\n   - Provide store credit for future purchases\\n\\n**Immediate compensation**: As an apology for this experience, I'd like to offer you [specific compensation - 20% off next order, free shipping, etc.]\\n\\n**Prevention**: I'll also add a note to your account to ensure priority handling for all future orders.\\n\\nYour satisfaction is our top priority, and I'm committed to making this right. What's your order number so I can get started immediately?"}}
```

## üìà Monitoring Training Progress

### Training Metrics to Watch

1. **Loss Trajectory**: Should decrease steadily
2. **Validation Loss**: Shouldn't diverge from training loss  
3. **Learning Rate**: May need adjustment if loss plateaus
4. **Memory Usage**: Monitor for M4 Pro limits

### Early Stopping Indicators

```bash
# Stop training if:
# - Validation loss increases for 3+ consecutive checkpoints
# - Training loss stops decreasing for 5+ checkpoints  
# - Memory usage exceeds 90% consistently
```

## üÜò Troubleshooting Common Issues

### "No valid training data found"
```bash
# Check your JSONL format
head -5 data/train.jsonl
python -c "import json; [json.loads(line) for line in open('data/train.jsonl')]"
```

### "Out of memory" errors
```bash
# Reduce batch size and sequence length
mlx-toolkit finetune model data/train.jsonl --batch-size 2 --max-seq-length 512
```

### Poor fine-tuning results
```bash
# 1. Increase training data quality and quantity
# 2. Try different learning rates: 5e-6, 1e-5, 2e-5
# 3. Increase LoRA rank: 8, 16, 32
# 4. More epochs: 5-10 for complex tasks
```

---

## üéâ Ready to Train!

You now have everything needed to create professional-quality training data for Llama models on your M4 Pro. Remember:

- **Quality > Quantity**: 500 excellent examples beat 5000 mediocre ones
- **Consistency**: Use the same format throughout your dataset  
- **Validation**: Always test with held-out data
- **Iteration**: Fine-tuning is an iterative process

**Happy fine-tuning!** üöÄ

---

*This guide is optimized for MLX Toolkit v0.1.0 and Llama-3.1-8B-Instruct-4bit on Apple Silicon M4 Pro*
"""
    
    readme_path = data_dir / "README.md"
    with open(readme_path, "w") as f:
        f.write(readme_content)
    
    console.print(f"‚úì Created comprehensive data guide: {readme_path}")


def validate_training_data(file_path: str) -> bool:
    """Validate training data format"""
    try:
        valid_count = 0
        total_count = 0
        
        with open(file_path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                total_count += 1
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # Check for valid formats
                    if ('instruction' in data and 'response' in data) or 'text' in data:
                        valid_count += 1
                    else:
                        console.print(f"[yellow]Line {line_num}: Missing required fields[/]")
                        
                except json.JSONDecodeError as e:
                    console.print(f"[red]Line {line_num}: Invalid JSON - {e}[/]")
        
        if valid_count == 0:
            console.print(f"[red]‚ùå No valid training examples found in {file_path}[/]")
            return False
        elif valid_count < total_count:
            console.print(f"[yellow]‚ö†Ô∏è {valid_count}/{total_count} valid examples in {file_path}[/]")
        else:
            console.print(f"[green]‚úì All {valid_count} examples are valid in {file_path}[/]")
        
        return valid_count > 0
        
    except FileNotFoundError:
        console.print(f"[red]‚ùå File not found: {file_path}[/]")
        return False
    except Exception as e:
        console.print(f"[red]‚ùå Error validating {file_path}: {e}[/]")
        return False


@main.command()
@click.option("--live", "-l", is_flag=True, help="Live updating dashboard")
@click.option("--refresh", "-r", default=2, help="Refresh interval in seconds")
def status(live, refresh):
    """üöÄ Beautiful system status dashboard"""
    if live:
        _run_live_status(refresh)
    else:
        _show_status_snapshot()


def _show_status_snapshot():
    """Show a single status snapshot"""
    layout = Layout()
    
    # Create layout sections
    layout.split_column(
        Layout(name="header", size=3),
        Layout(name="main"),
        Layout(name="footer", size=3)
    )
    
    layout["main"].split_row(
        Layout(name="left"),
        Layout(name="right")
    )
    
    # Header with beautiful title
    header = Panel(
        Align.center("üöÄ [bold cyan]MLX TOOLKIT STATUS DASHBOARD[/] üöÄ"),
        style="bold blue",
        border_style="blue"
    )
    layout["header"].update(header)
    
    # System info
    system_info = _get_system_info()
    layout["left"].update(system_info)
    
    # Project info  
    project_info = _get_project_info()
    layout["right"].update(project_info)
    
    # Footer with tips
    footer = Panel(
        "[dim]üí° Use --live for real-time updates | Press Ctrl+C to exit[/]",
        style="dim",
        border_style="dim"
    )
    layout["footer"].update(footer)
    
    console.print(layout)


def _run_live_status(refresh_interval):
    """Run live updating status dashboard"""
    console.print("[bold green]üî¥ LIVE STATUS DASHBOARD[/] - Press Ctrl+C to exit")
    
    try:
        with Live(console=console, refresh_per_second=1/refresh_interval) as live:
            while True:
                layout = Layout()
                
                layout.split_column(
                    Layout(name="header", size=3),
                    Layout(name="main"),
                    Layout(name="footer", size=3)
                )
                
                layout["main"].split_row(
                    Layout(name="left"),
                    Layout(name="right")
                )
                
                # Animated header
                current_time = time.strftime("%H:%M:%S")
                header = Panel(
                    Align.center(f"üöÄ [bold cyan]LIVE MLX DASHBOARD[/] üöÄ [dim]{current_time}[/]"),
                    style="bold blue",
                    border_style="blue"
                )
                layout["header"].update(header)
                
                # Live system info
                system_info = _get_system_info()
                layout["left"].update(system_info)
                
                # Live project info
                project_info = _get_project_info()
                layout["right"].update(project_info)
                
                # Live footer with stats
                footer = Panel(
                    f"[dim]üîÑ Refreshing every {refresh_interval}s | üíæ RAM: {psutil.virtual_memory().percent:.1f}% | üî• CPU: {psutil.cpu_percent():.1f}%[/]",
                    style="dim",
                    border_style="dim"
                )
                layout["footer"].update(footer)
                
                live.update(layout)
                time.sleep(refresh_interval)
                
    except KeyboardInterrupt:
        console.print("\n[yellow]üëã Live dashboard stopped[/]")


def _get_system_info():
    """Get beautiful system information panel"""
    # System specs
    cpu_percent = psutil.cpu_percent(interval=0.1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # Create system info table
    table = Table(title="üñ•Ô∏è  System Status", show_header=True, header_style="bold magenta")
    table.add_column("Metric", style="cyan", no_wrap=True)
    table.add_column("Value", style="green")
    table.add_column("Visual", style="yellow")
    
    # CPU usage with bar
    cpu_bar = "‚ñà" * int(cpu_percent / 5) + "‚ñë" * (20 - int(cpu_percent / 5))
    cpu_color = "red" if cpu_percent > 80 else "yellow" if cpu_percent > 60 else "green"
    table.add_row("üî• CPU", f"{cpu_percent:.1f}%", f"[{cpu_color}]{cpu_bar}[/]")
    
    # Memory usage with bar
    mem_bar = "‚ñà" * int(memory.percent / 5) + "‚ñë" * (20 - int(memory.percent / 5))
    mem_color = "red" if memory.percent > 80 else "yellow" if memory.percent > 60 else "green"
    table.add_row("üíæ RAM", f"{memory.percent:.1f}%", f"[{mem_color}]{mem_bar}[/]")
    
    # Disk usage
    disk_bar = "‚ñà" * int(disk.percent / 5) + "‚ñë" * (20 - int(disk.percent / 5))
    disk_color = "red" if disk.percent > 80 else "yellow" if disk.percent > 60 else "green"
    table.add_row("üíΩ Disk", f"{disk.percent:.1f}%", f"[{disk_color}]{disk_bar}[/]")
    
    # Python info
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    table.add_row("üêç Python", python_version, "‚úÖ")
    
    return Panel(table, border_style="magenta")


def _get_project_info():
    """Get beautiful project information panel"""
    tree = Tree("üìÅ Project Status", style="bold blue")
    
    # Check if in MLX project
    if Path("venv").exists():
        venv_node = tree.add("üîã Virtual Environment", style="green")
        venv_node.add("‚úÖ Active")
        
        # Check installed packages
        packages_node = tree.add("üì¶ MLX Packages", style="cyan")
        
        # Package mapping: display_name -> import_name
        key_packages = {
            "mlx": "mlx",
            "mlx-lm": "mlx_lm", 
            "transformers": "transformers",
            "rich": "rich",
            "psutil": "psutil"
        }
        
        for display_name, import_name in key_packages.items():
            try:
                result = subprocess.run([
                    "venv/bin/python" if os.name != 'nt' else "venv/Scripts/python",
                    "-c", f"import {import_name}; print('‚úÖ')"
                ], capture_output=True, text=True, timeout=3)
                
                if result.returncode == 0:
                    packages_node.add(f"‚úÖ {display_name}", style="green")
                else:
                    # Try alternative check with pip list
                    pip_result = subprocess.run([
                        "venv/bin/pip" if os.name != 'nt' else "venv/Scripts/pip",
                        "show", display_name
                    ], capture_output=True, text=True, timeout=3)
                    
                    if pip_result.returncode == 0:
                        packages_node.add(f"‚úÖ {display_name} (pip)", style="green")
                    else:
                        packages_node.add(f"‚ùå {display_name}", style="red")
            except Exception as e:
                packages_node.add(f"‚ùì {display_name}", style="yellow")
    else:
        tree.add("‚ùå Not in MLX project", style="red")
        tree.add("üí° Run: mlx-toolkit init <project-name>", style="yellow")
    
    # Check models directory
    if Path("models").exists():
        models_node = tree.add("ü§ñ Models", style="green")
        model_dirs = list(Path("models").glob("*"))
        if model_dirs:
            for model_dir in model_dirs[:5]:  # Show first 5
                size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024**3)
                models_node.add(f"üìÅ {model_dir.name} ({size:.1f}GB)", style="cyan")
            if len(model_dirs) > 5:
                models_node.add(f"... and {len(model_dirs)-5} more", style="dim")
        else:
            models_node.add("üì• No models downloaded", style="yellow")
    else:
        tree.add("üì• No models directory", style="yellow")
    
    # Check training data
    if Path("data").exists():
        data_node = tree.add("üìä Training Data", style="green")
        data_files = list(Path("data").glob("*.jsonl"))
        if data_files:
            for data_file in data_files[:3]:
                lines = sum(1 for _ in open(data_file)) if data_file.stat().st_size < 10*1024*1024 else "Large"
                data_node.add(f"üìÑ {data_file.name} ({lines} lines)", style="cyan")
        else:
            data_node.add("üìù No JSONL files found", style="yellow")
    
    return Panel(tree, title="üìã Project Overview", border_style="blue")


@main.command()
@click.option("--search", "-s", help="Search models by name")
@click.option("--filter", "-f", type=click.Choice(["all", "downloaded", "available"]), default="all", help="Filter models")
@click.option("--sort", type=click.Choice(["name", "size", "downloads"]), default="name", help="Sort by")
def models(search, filter, sort):
    """ü§ñ Beautiful model browser and management"""
    console.print("\n")
    console.print(Panel(
        Align.center("ü§ñ [bold cyan]MLX MODEL BROWSER[/] ü§ñ"),
        style="bold blue",
        border_style="blue"
    ))
    
    # Get model information
    models_data = _get_models_data(search, filter, sort)
    
    # Create beautiful table
    table = Table(
        title="üìä Available Models", 
        show_header=True, 
        header_style="bold magenta",
        title_style="bold cyan"
    )
    
    table.add_column("ü§ñ Model", style="cyan", no_wrap=True, min_width=25)
    table.add_column("üìè Size", style="green", justify="right")
    table.add_column("üöÄ Type", style="yellow")
    table.add_column("üìä Status", style="blue")
    table.add_column("‚≠ê Rating", justify="center")
    table.add_column("üîó Actions", style="magenta")
    
    for model in models_data:
        # Status with color coding
        if model["downloaded"]:
            status = "[green]‚úÖ Downloaded[/]"
            actions = "[red]üóëÔ∏è  Delete[/]"
        else:
            status = "[yellow]üì• Available[/]"
            actions = f"[green]‚¨áÔ∏è  Download ({model['size']})[/]"
        
        # Star rating
        stars = "‚≠ê" * model["rating"] + "‚òÜ" * (5 - model["rating"])
        
        table.add_row(
            model["name"],
            model["size"],
            model["type"],
            status,
            stars,
            actions
        )
    
    console.print(table)
    console.print("\n")
    
    # Interactive selection
    if models_data:
        console.print(Panel(
            "[bold yellow]üí° Quick Actions:[/]\n\n"
            "[cyan]mlx-toolkit download-model <model-name>[/] - Download a model\n"
            "[cyan]mlx-toolkit chat models/<model-name>[/] - Chat with downloaded model\n"
            "[cyan]mlx-toolkit lora models/<model-name> data/train.jsonl[/] - Fine-tune with LoRA\n\n"
            "[dim]üîç Use --search to filter | --filter to show downloaded only[/]",
            title="üöÄ Quick Start Guide",
            border_style="green"
        ))
    
    # Show disk usage summary
    _show_models_summary()


def _get_models_data(search=None, filter_type="all", sort_by="name"):
    """Get models data with filtering and sorting"""
    models = []
    
    # Popular MLX Community models
    popular_models = [
        {
            "name": "Llama-3.1-8B-Instruct-4bit",
            "size": "4.9GB",
            "type": "Instruct",
            "rating": 5,
            "downloads": 50000
        },
        {
            "name": "Mistral-7B-Instruct-v0.3-4bit", 
            "size": "4.1GB",
            "type": "Instruct",
            "rating": 4,
            "downloads": 35000
        },
        {
            "name": "Phi-3.5-mini-instruct-4bit",
            "size": "2.4GB", 
            "type": "Mini",
            "rating": 4,
            "downloads": 25000
        },
        {
            "name": "Qwen2-7B-Instruct-4bit",
            "size": "4.3GB",
            "type": "Multilingual", 
            "rating": 4,
            "downloads": 20000
        },
        {
            "name": "CodeLlama-7b-Instruct-hf-4bit-mlx-2",
            "size": "4.0GB",
            "type": "Code",
            "rating": 5,
            "downloads": 30000
        }
    ]
    
    # Check which models are downloaded
    models_dir = Path("models")
    downloaded_models = set()
    if models_dir.exists():
        downloaded_models = {d.name for d in models_dir.iterdir() if d.is_dir()}
    
    for model in popular_models:
        model["downloaded"] = model["name"] in downloaded_models
        
        # Apply search filter
        if search and search.lower() not in model["name"].lower():
            continue
            
        # Apply type filter
        if filter_type == "downloaded" and not model["downloaded"]:
            continue
        elif filter_type == "available" and model["downloaded"]:
            continue
            
        models.append(model)
    
    # Sort models
    if sort_by == "size":
        models.sort(key=lambda x: float(x["size"].replace("GB", "")))
    elif sort_by == "downloads":
        models.sort(key=lambda x: x["downloads"], reverse=True)
    else:  # name
        models.sort(key=lambda x: x["name"])
    
    return models


def _show_models_summary():
    """Show models disk usage summary"""
    models_dir = Path("models")
    if not models_dir.exists():
        return
        
    total_size = 0
    model_count = 0
    
    for model_dir in models_dir.iterdir():
        if model_dir.is_dir():
            model_count += 1
            size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
            total_size += size
    
    if model_count > 0:
        total_gb = total_size / (1024**3)
        
        # Create summary panel
        summary = f"""[bold cyan]üìä Models Summary[/]

ü§ñ Models Downloaded: [green]{model_count}[/]
üíæ Total Disk Usage: [yellow]{total_gb:.1f} GB[/]
üìÅ Storage Location: [dim]./models/[/]

[dim]üí° To free up space, delete unused models with:[/]
[cyan]rm -rf models/<model-name>[/]"""
        
        console.print(Panel(summary, title="üíæ Storage Info", border_style="yellow"))


@main.command()
@click.option("--live", "-l", is_flag=True, help="Live monitoring mode")
@click.option("--log-file", default="training.log", help="Training log file to monitor")
def train_monitor(live, log_file):
    """üî• Real-time training monitor with live charts and metrics"""
    if live:
        _run_live_training_monitor(log_file)
    else:
        _show_training_summary(log_file)


def _run_live_training_monitor(log_file):
    """Epic live training dashboard with smooth updates"""
    console.print("[bold red]üî• LIVE TRAINING MONITOR[/] - Press Ctrl+C to exit\n")
    
    # Initialize tracking
    loss_history = []
    metrics_history = []
    start_time = time.time()
    last_file_size = 0
    
    try:
        with Live(console=console, refresh_per_second=3) as live:
            while True:
                # Create main layout with better proportions
                layout = Layout()
                layout.split_column(
                    Layout(name="header", size=3),
                    Layout(name="main", size=20),
                    Layout(name="footer", size=2)
                )
                
                layout["main"].split_row(
                    Layout(name="charts", ratio=3),
                    Layout(name="metrics", ratio=2)
                )
                
                # Header with live time
                elapsed = timedelta(seconds=int(time.time() - start_time))
                header = Panel(
                    Align.center(f"üî• [bold red]LIVE TRAINING MONITOR[/] üî• [dim]Monitor Time: {elapsed}[/]"),
                    style="bold red",
                    border_style="red",
                    padding=(0, 1)
                )
                layout["header"].update(header)
                
                # Parse latest training data (only if file changed)
                try:
                    if Path(log_file).exists():
                        current_size = Path(log_file).stat().st_size
                        if current_size != last_file_size:
                            latest_data = _parse_training_log(log_file)
                            if latest_data:
                                new_losses = latest_data.get('losses', [])
                                if new_losses:
                                    loss_history = new_losses  # Replace with fresh data
                                    # Keep only last 150 points for performance
                                    loss_history = loss_history[-150:]
                            last_file_size = current_size
                except Exception:
                    pass
                
                # Create live loss chart
                chart_panel = _create_loss_chart(loss_history)
                layout["charts"].update(chart_panel)
                
                # Create metrics panel
                metrics_panel = _create_training_metrics(loss_history, metrics_history, start_time)
                layout["metrics"].update(metrics_panel)
                
                # Footer with compact system stats
                cpu_percent = psutil.cpu_percent(interval=None)
                memory_percent = psutil.virtual_memory().percent
                points_count = len(loss_history)
                status = "üî• Training" if points_count > 0 else "‚è≥ Waiting"
                
                footer_text = f"[dim]{status} | CPU: {cpu_percent:.0f}% | RAM: {memory_percent:.0f}% | Data Points: {points_count}[/]"
                footer = Panel(footer_text, style="dim", border_style="dim", padding=(0, 1))
                layout["footer"].update(footer)
                
                live.update(layout)
                time.sleep(0.33)  # ~3 FPS
                
    except KeyboardInterrupt:
        console.print("\n[yellow]üèÅ Training monitor stopped[/]")
        if loss_history:
            console.print(f"[green]üìä Monitored {len(loss_history)} training steps[/]")


def _create_loss_chart(loss_history):
    """Create epic ASCII loss chart with better scaling"""
    if not loss_history:
        return Panel(
            Align.center("[dim]üìà Waiting for training data...\n\nStart training to see live loss curves![/]"),
            title="üìà Loss Curve",
            border_style="blue"
        )
    
    # Dynamic chart sizing
    chart_height = 12
    chart_width = min(80, max(40, len(loss_history)))  # Adaptive width
    
    if len(loss_history) < 2:
        chart_content = "[dim]Collecting data...[/]\n" * (chart_height - 1) + "[dim]Collecting data...[/]"
        return Panel(chart_content, title="üìà Live Loss Curve (Real-time)", border_style="green", padding=(0, 1))
    else:
        # Better normalization with padding
        min_loss = min(loss_history)
        max_loss = max(loss_history)
        loss_range = max_loss - min_loss if max_loss > min_loss else max_loss * 0.1
        
        # Add 10% padding to make chart more readable
        padding = loss_range * 0.1
        chart_min = min_loss - padding
        chart_max = max_loss + padding
        chart_range = chart_max - chart_min
        
        # Resample data points for chart width
        if len(loss_history) > chart_width:
            # Average values in buckets for smoother curve
            bucket_size = len(loss_history) / chart_width
            resampled = []
            for i in range(chart_width):
                start_idx = int(i * bucket_size)
                end_idx = int((i + 1) * bucket_size)
                bucket_losses = loss_history[start_idx:end_idx]
                if bucket_losses:
                    resampled.append(statistics.mean(bucket_losses))
            loss_data = resampled
        else:
            loss_data = loss_history
        
        # Create chart grid
        chart_lines = []
        for row in range(chart_height):
            line = ""
            y_value = chart_max - (row / (chart_height - 1)) * chart_range
            
            for col in range(len(loss_data)):
                loss = loss_data[col]
                
                # Better point plotting - use different characters for better curve
                y_diff = abs(loss - y_value)
                threshold = chart_range / (chart_height * 2)
                
                if y_diff < threshold:
                    # Use different characters based on how close to the point
                    if y_diff < threshold * 0.3:
                        line += "[bright_green]‚ñà[/]"
                    elif y_diff < threshold * 0.7:
                        line += "[green]‚ñì[/]"
                    else:
                        line += "[green]‚ñë[/]"
                else:
                    line += "[dim]¬∑[/]"
            
            # Pad line to exact chart width (strip markup and count real chars)
            import re
            clean_line = re.sub(r'\[.*?\]', '', line)  # Remove all Rich markup
            actual_width = len(clean_line)
            if actual_width < chart_width:
                line += " " * (chart_width - actual_width)
            
            # Y-axis labels with perfect alignment
            if row == 0:
                label = f"[cyan]{chart_max:.4f}[/]"
            elif row == chart_height - 1:
                label = f"[cyan]{chart_min:.4f}[/]"
            elif row == chart_height // 2:
                label = f"[cyan]{(chart_max + chart_min) / 2:.4f}[/]"
            else:
                label = ""
            
            # Ensure consistent Y-axis width (8 chars)
            if label:
                # Pad or truncate to exactly 8 characters
                clean_label = label.replace('[cyan]', '').replace('[/]', '')
                if len(clean_label) > 8:
                    clean_label = clean_label[:8]
                else:
                    clean_label = clean_label.ljust(8)
                label = f"[cyan]{clean_label}[/]"
            else:
                label = "        "  # 8 spaces
            
            chart_lines.append(f"{label}‚îÇ{line}")
        
        # X-axis with perfect alignment
        x_axis = "        ‚îî" + "‚îÄ" * min(chart_width, 50) + f" Steps: {len(loss_history)}"
        chart_lines.append(x_axis)
        
        # Enhanced statistics
        current_loss = loss_history[-1]
        best_loss = min(loss_history)
        worst_loss = max(loss_history)
        avg_loss = statistics.mean(loss_history[-min(20, len(loss_history)):])
        
        # Calculate trend
        if len(loss_history) >= 5:
            recent_avg = statistics.mean(loss_history[-5:])
            older_avg = statistics.mean(loss_history[-10:-5]) if len(loss_history) >= 10 else loss_history[0]
            trend = "üìâ Improving" if recent_avg < older_avg else "üìà Rising"
            trend_color = "green" if recent_avg < older_avg else "red"
        else:
            trend = "üîÑ Starting"
            trend_color = "yellow"
        
        stats = f"""
[bold green]üìä Loss Analytics[/]
Current: [yellow]{current_loss:.6f}[/]  Best: [green]{best_loss:.6f}[/]  Worst: [red]{worst_loss:.6f}[/]
Avg(20): [cyan]{avg_loss:.6f}[/]  Range: [dim]{worst_loss - best_loss:.6f}[/]  Trend: [{trend_color}]{trend}[/]
Data Points: [dim]{len(loss_history)}[/]  Chart Width: [dim]{chart_width}[/]"""
        
        chart_lines.append(stats)
    
    chart_content = "\n".join(chart_lines)
    return Panel(chart_content, title="üìà Live Loss Curve (Real-time)", border_style="green", padding=(0, 1))


def _create_training_metrics(loss_history, metrics_history, start_time):
    """Create detailed training metrics panel"""
    layout = Layout()
    layout.split_column(
        Layout(name="metrics", ratio=1),
        Layout(name="system", ratio=1)
    )
    
    # Use simple text formatting instead of tables to eliminate spacing issues
    training_lines = []
    system_lines = []
    
    # Calculate training metrics
    elapsed_time = time.time() - start_time
    steps = len(loss_history)
    
    if steps > 0:
        steps_per_sec = steps / elapsed_time if elapsed_time > 0 else 0
        current_loss = loss_history[-1] if loss_history else 0
        
        # Advanced trend analysis
        if len(loss_history) >= 10:
            recent_losses = loss_history[-5:]
            older_losses = loss_history[-10:-5]
            recent_avg = statistics.mean(recent_losses)
            older_avg = statistics.mean(older_losses)
            trend_change = ((recent_avg - older_avg) / older_avg) * 100
            
            if trend_change < -5:
                improving = "üìâ Improving"
            elif trend_change > 5:
                improving = "üìà Degrading"
            else:
                improving = "üìä Stable"
                
            # Loss velocity (rate of change)
            if len(loss_history) >= 20:
                loss_velocity = (statistics.mean(loss_history[-5:]) - statistics.mean(loss_history[-20:-15])) / 15
                velocity_str = f"{loss_velocity:.6f}/step"
            else:
                velocity_str = "Calculating..."
        else:
            improving = "üîÑ Starting"
            velocity_str = "N/A"
        
        # Time estimates
        steps_remaining = max(0, 1000 - steps)
        eta_seconds = steps_remaining / steps_per_sec if steps_per_sec > 0 else 0
        eta_str = str(timedelta(seconds=int(eta_seconds))) if eta_seconds < 86400 else "Unknown"
        
        # Best/worst performance
        best_loss = min(loss_history)
        best_step = loss_history.index(best_loss) + 1
        improvement = ((current_loss - best_loss) / best_loss) * 100 if best_loss > 0 else 0
        
        speed_status = "üî• Fast" if steps_per_sec > 0.5 else "üêå Slow"
        
        # Build training metrics as simple text
        training_lines.extend([
            f"[cyan]‚è±Ô∏è Elapsed[/]     [yellow]{timedelta(seconds=int(elapsed_time))}[/]     [green]üü¢ Running[/]",
            f"[cyan]üöÄ Steps[/]       [yellow]{steps:,}[/]     [green]üìä Tracking[/]",
            f"[cyan]‚ö° Speed[/]       [yellow]{steps_per_sec:.2f}/sec[/]     [green]{speed_status}[/]",
            f"[cyan]üìâ Current[/]     [yellow]{current_loss:.6f}[/]     [green]{improving}[/]",
            f"[cyan]üèÜ Best[/]        [yellow]{best_loss:.6f} (#{best_step})[/]     [green]‚≠ê Record[/]",
            f"[cyan]üéØ Change[/]      [yellow]{improvement:+.2f}%[/]     [green]üìà Tracking[/]",
            f"[cyan]üí® Velocity[/]    [yellow]{velocity_str}[/]     [green]üìä Analysis[/]",
            f"[cyan]‚è∞ ETA[/]         [yellow]{eta_str}[/]     [green]üéØ Estimate[/]",
        ])
        
    else:
        training_lines.extend([
            f"[cyan]üì° Status[/]      [yellow]Waiting...[/]     [green]üîç Monitoring[/]",
            f"[cyan]üí° Tip[/]         [yellow]Start training![/]     [green]üìñ Ready[/]",
        ])
    
    # System metrics
    cpu_percent = psutil.cpu_percent(interval=0.1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # CPU status
    cpu_status = "üî• Hot" if cpu_percent > 80 else "‚ö° Good" if cpu_percent > 50 else "üò¥ Idle"
    cpu_bar = "‚ñà" * int(cpu_percent // 10) + "‚ñë" * (10 - int(cpu_percent // 10))
    
    # Memory status  
    mem_percent = memory.percent
    mem_status = "üî¥ Critical" if mem_percent > 90 else "üü° High" if mem_percent > 75 else "üü¢ Normal"
    mem_bar = "‚ñà" * int(mem_percent // 10) + "‚ñë" * (10 - int(mem_percent // 10))
    
    # Disk status
    disk_percent = (disk.used / disk.total) * 100
    disk_status = "üü° Full" if disk_percent > 90 else "üü¢ OK"
    
    # Build system metrics as simple text
    system_lines.extend([
        f"[blue]üî• CPU[/]         [yellow]{cpu_percent:.1f}% {cpu_bar}[/]     [green]{cpu_status}[/]",
        f"[blue]üíæ Memory[/]      [yellow]{mem_percent:.1f}% {mem_bar}[/]     [green]{mem_status}[/]",
        f"[blue]üíΩ Disk[/]        [yellow]{disk_percent:.1f}%[/]     [green]{disk_status}[/]",
        f"[blue]üêç Python[/]      [yellow]PID {os.getpid()}[/]     [green]üü¢ Active[/]",
    ])
    
    # Temperature (if available on macOS)
    try:
        if hasattr(psutil, 'sensors_temperatures'):
            temps = psutil.sensors_temperatures()
            if temps:
                avg_temp = statistics.mean([temp.current for sensors in temps.values() for temp in sensors])
                temp_status = "üî• Hot" if avg_temp > 70 else "üå°Ô∏è Warm" if avg_temp > 50 else "‚ùÑÔ∏è Cool"
                system_lines.append(f"[blue]üå°Ô∏è Temp[/]        [yellow]{avg_temp:.1f}¬∞C[/]     [green]{temp_status}[/]")
    except:
        pass
    
    # Create panels with simple text - no table spacing issues
    training_content = "\n".join(training_lines)
    system_content = "\n".join(system_lines)
    
    layout["metrics"].update(Panel(
        training_content, 
        title="üöÄ Training Metrics",
        border_style="cyan", 
        padding=(0, 0),
        title_align="left"
    ))
    layout["system"].update(Panel(
        system_content, 
        title="üñ•Ô∏è System Status",
        border_style="blue", 
        padding=(0, 0),
        title_align="left"
    ))
    
    return layout


def _parse_training_log(log_file):
    """Parse MLX-LM training log for comprehensive metrics"""
    try:
        if not Path(log_file).exists():
            return None
            
        with open(log_file, 'r') as f:
            lines = f.readlines()
        
        losses = []
        metrics = []
        learning_rates = []
        tokens_per_sec = []
        iterations_per_sec = []
        
        # Parse lines for MLX-LM specific output patterns
        import re
        
        for line in lines[-50:]:  # Check more recent lines for better data
            line = line.strip()
            
            # MLX-LM training output examples:
            # "Iter 10: Train loss 2.345, It/sec 1.23, Tokens/sec 456.78, Learning rate: 1e-4"
            # "Iter 100: Val loss 1.234, Train loss 2.345"
            
            # Primary pattern for comprehensive training info
            full_match = re.search(
                r'Iter\s+(\d+):\s+(?:.*?loss\s+(\d+\.\d+))(?:.*?It/sec\s+(\d+\.\d+))?(?:.*?Tokens/sec\s+(\d+\.\d+))?(?:.*?Learning\s+rate:?\s+([\d.e-]+))?', 
                line, re.IGNORECASE
            )
            if full_match:
                iter_num = int(full_match.group(1))
                loss_val = float(full_match.group(2))
                it_sec = float(full_match.group(3)) if full_match.group(3) else None
                tok_sec = float(full_match.group(4)) if full_match.group(4) else None
                lr = float(full_match.group(5)) if full_match.group(5) else None
                
                if 0.001 <= loss_val <= 100:
                    losses.append(loss_val)
                    if it_sec: iterations_per_sec.append(it_sec)
                    if tok_sec: tokens_per_sec.append(tok_sec)
                    if lr: learning_rates.append(lr)
                    continue
            
            # Simpler pattern for just losses
            iter_match = re.search(r'Iter\s+(\d+):\s+.*?loss\s+(\d+\.\d+)', line, re.IGNORECASE)
            if iter_match:
                iter_num = int(iter_match.group(1))
                loss_val = float(iter_match.group(2))
                if 0.001 <= loss_val <= 100:
                    losses.append(loss_val)
                    continue
            
            # Alternative patterns
            loss_match = re.search(r'(?:train\s+)?loss:?\s+(\d+\.\d+)', line, re.IGNORECASE)
            if loss_match:
                loss_val = float(loss_match.group(1))
                if 0.001 <= loss_val <= 100:
                    losses.append(loss_val)
                    continue
            
            # Extract performance metrics separately
            if 'it/sec' in line.lower():
                it_match = re.search(r'it/sec\s+(\d+\.\d+)', line, re.IGNORECASE)
                if it_match:
                    iterations_per_sec.append(float(it_match.group(1)))
            
            if 'tokens/sec' in line.lower():
                tok_match = re.search(r'tokens/sec\s+(\d+\.\d+)', line, re.IGNORECASE)
                if tok_match:
                    tokens_per_sec.append(float(tok_match.group(1)))
            
            if 'learning rate' in line.lower():
                lr_match = re.search(r'learning\s+rate:?\s+([\d.e-]+)', line, re.IGNORECASE)
                if lr_match:
                    learning_rates.append(float(lr_match.group(1)))
        
        # Compile comprehensive metrics
        result = {
            'losses': losses,
            'metrics': metrics,
            'learning_rates': learning_rates,
            'tokens_per_sec': tokens_per_sec,
            'iterations_per_sec': iterations_per_sec,
            'total_lines': len(lines),
            'last_update': time.time()
        }
        
        return result if losses else None
        
    except Exception as e:
        return None


def _show_training_summary(log_file):
    """Show training summary from log"""
    if Path(log_file).exists():
        # Parse existing log
        data = _parse_training_log(log_file)
        if data and data['losses']:
            losses = data['losses']
            console.print(Panel(
                f"[bold cyan]üìä Training Summary[/]\n\n"
                f"Log file: [yellow]{log_file}[/]\n"
                f"Total loss points: [green]{len(losses)}[/]\n"
                f"Latest loss: [yellow]{losses[-1]:.6f}[/]\n"
                f"Best loss: [green]{min(losses):.6f}[/]\n"
                f"Average loss: [cyan]{sum(losses)/len(losses):.6f}[/]\n\n"
                f"Use [cyan]--live[/] for real-time monitoring!",
                title="üìà Training Progress",
                border_style="green"
            ))
        else:
            console.print(Panel(
                f"[bold yellow]üìã Training Log Found[/]\n\n"
                f"Log file: [yellow]{log_file}[/]\n"
                f"Status: [dim]No loss data detected yet[/]\n\n"
                f"üí° The log might contain training output that hasn't\n"
                f"   started showing loss values yet.\n\n"
                f"Use [cyan]--live[/] for real-time monitoring!",
                title="Training Monitor",
                border_style="yellow"
            ))
    else:
        console.print(Panel(
            f"[bold cyan]üìä Training Monitor[/]\n\n"
            f"No training log found: [dim]{log_file}[/]\n\n"
            f"üöÄ Start training with:\n"
            f"[cyan]mlx-toolkit lora <model> <data>[/]\n\n"
            f"Then monitor with:\n"
            f"[cyan]mlx-toolkit train-monitor --live[/]",
            title="üîç No Training Detected",
            border_style="blue"
        ))


@main.command()
@click.argument("model_path")
@click.option("--prompt", default="The quick brown fox", help="Test prompt")
@click.option("--iterations", default=10, help="Number of test iterations")
@click.option("--max-tokens", default=100, help="Max tokens to generate")
def benchmark(model_path, prompt, iterations, max_tokens):
    """‚ö° Comprehensive model benchmarking and performance testing"""
    console.print("\n")
    console.print(Panel(
        Align.center("‚ö° [bold yellow]MLX MODEL BENCHMARK[/] ‚ö°"),
        style="bold yellow",
        border_style="yellow"
    ))
    
    # Run comprehensive benchmark
    results = _run_model_benchmark(model_path, prompt, iterations, max_tokens)
    
    # Display beautiful results
    _display_benchmark_results(results)


def _run_model_benchmark(model_path, prompt, iterations, max_tokens):
    """Run comprehensive model benchmarks"""
    console.print(f"[yellow]üî• Benchmarking {model_path}...[/]")
    
    results = {
        'model_path': model_path,
        'prompt': prompt,
        'iterations': iterations,
        'max_tokens': max_tokens,
        'timestamp': datetime.now(),
        'system_info': {
            'cpu_count': psutil.cpu_count(),
            'memory_gb': psutil.virtual_memory().total / (1024**3),
            'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        }
    }
    
    # Check if model exists
    if not Path(model_path).exists():
        console.print(f"[red]‚ùå Model not found: {model_path}[/]")
        return results
    
    # Speed benchmark
    with console.status("[bold yellow]‚ö° Running speed benchmark..."):
        speed_results = _benchmark_speed(model_path, prompt, iterations, max_tokens)
        results.update(speed_results)
    
    # Memory benchmark
    with console.status("[bold blue]üíæ Testing memory usage..."):
        memory_results = _benchmark_memory(model_path, prompt)
        results.update(memory_results)
    
    # Quality benchmark (if we have test data)
    # with console.status("[bold green]üìä Evaluating quality..."):
    #     quality_results = _benchmark_quality(model_path)
    #     results.update(quality_results)
    
    return results


def _benchmark_speed(model_path, prompt, iterations, max_tokens):
    """Benchmark model speed"""
    times = []
    token_counts = []
    
    # Use project's Python environment
    if os.name == 'nt':
        python_path = "venv/Scripts/python"
    else:
        python_path = "venv/bin/python"
    
    for i in range(iterations):
        start_time = time.time()
        
        try:
            # Run generation
            cmd = [
                python_path, "-c",
                f"""
import time
start = time.time()
try:
    from mlx_lm import load, generate
    model, tokenizer = load('{model_path}')
    response = generate(model, tokenizer, prompt='{prompt}', max_tokens={max_tokens}, verbose=False)
    end = time.time()
    print(f'TIME:{{end-start:.3f}}')
    print(f'TOKENS:{{len(response.split())}}')
    print(f'RESPONSE:{{response[:100]}}...')
except Exception as e:
    print(f'ERROR:{{e}}')
"""
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                output = result.stdout
                # Parse timing info
                for line in output.split('\n'):
                    if line.startswith('TIME:'):
                        elapsed = float(line.split(':')[1])
                        times.append(elapsed)
                    elif line.startswith('TOKENS:'):
                        tokens = int(line.split(':')[1])
                        token_counts.append(tokens)
            else:
                console.print(f"[red]‚ùå Benchmark iteration {i+1} failed[/]")
                
        except subprocess.TimeoutExpired:
            console.print(f"[yellow]‚è∞ Benchmark iteration {i+1} timed out[/]")
        except Exception as e:
            console.print(f"[red]‚ùå Error in iteration {i+1}: {e}[/]")
    
    # Calculate statistics
    if times:
        avg_time = statistics.mean(times)
        avg_tokens = statistics.mean(token_counts) if token_counts else 0
        tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0
        
        return {
            'speed': {
                'avg_time': avg_time,
                'min_time': min(times),
                'max_time': max(times),
                'avg_tokens': avg_tokens,
                'tokens_per_sec': tokens_per_sec,
                'successful_runs': len(times),
                'total_runs': iterations
            }
        }
    else:
        return {'speed': {'error': 'No successful benchmark runs'}}


def _benchmark_memory(model_path, prompt):
    """Benchmark memory usage"""
    # Monitor memory during model loading and inference
    initial_memory = psutil.virtual_memory().used
    
    # This is a simplified version - real implementation would need more sophisticated monitoring
    peak_memory = psutil.virtual_memory().used
    memory_used = (peak_memory - initial_memory) / (1024**2)  # MB
    
    return {
        'memory': {
            'peak_usage_mb': memory_used,
            'initial_mb': initial_memory / (1024**2),
            'available_mb': psutil.virtual_memory().available / (1024**2)
        }
    }


def _display_benchmark_results(results):
    """Display beautiful benchmark results"""
    
    # System info
    system_table = Table(title="üñ•Ô∏è System Information", show_header=True, header_style="bold blue")
    system_table.add_column("Component", style="cyan")
    system_table.add_column("Specification", style="green")
    
    sys_info = results.get('system_info', {})
    system_table.add_row("üî• CPU Cores", str(sys_info.get('cpu_count', 'Unknown')))
    system_table.add_row("üíæ Memory", f"{sys_info.get('memory_gb', 0):.1f} GB")
    system_table.add_row("üêç Python", sys_info.get('python_version', 'Unknown'))
    system_table.add_row("ü§ñ Model", results.get('model_path', 'Unknown'))
    
    console.print(system_table)
    console.print()
    
    # Speed results
    speed = results.get('speed', {})
    if 'error' not in speed:
        speed_table = Table(title="‚ö° Speed Benchmark Results", show_header=True, header_style="bold yellow")
        speed_table.add_column("Metric", style="cyan")
        speed_table.add_column("Value", style="yellow", justify="right")
        speed_table.add_column("Rating", style="green")
        
        avg_time = speed.get('avg_time', 0)
        tokens_per_sec = speed.get('tokens_per_sec', 0)
        successful = speed.get('successful_runs', 0)
        total = speed.get('total_runs', 0)
        
        speed_table.add_row("‚è±Ô∏è Avg Time", f"{avg_time:.3f}s", "‚ö° Fast" if avg_time < 2 else "üêå Slow")
        speed_table.add_row("üöÄ Tokens/sec", f"{tokens_per_sec:.2f}", "üî• Blazing" if tokens_per_sec > 50 else "üìà Good")
        speed_table.add_row("‚úÖ Success Rate", f"{successful}/{total}", "üíØ Perfect" if successful == total else "‚ö†Ô∏è Issues")
        speed_table.add_row("üìä Avg Tokens", f"{speed.get('avg_tokens', 0):.0f}", "üìù Generated")
        
        console.print(speed_table)
    else:
        console.print(Panel(f"[red]‚ùå Speed benchmark failed: {speed['error']}[/]", border_style="red"))
    
    console.print()
    
    # Memory results
    memory = results.get('memory', {})
    memory_table = Table(title="üíæ Memory Usage", show_header=True, header_style="bold blue")
    memory_table.add_column("Metric", style="cyan")
    memory_table.add_column("Value", style="blue", justify="right")
    memory_table.add_column("Status", style="green")
    
    peak_mb = memory.get('peak_usage_mb', 0)
    available_mb = memory.get('available_mb', 0)
    
    memory_table.add_row("üìà Peak Usage", f"{peak_mb:.1f} MB", "‚úÖ Efficient" if peak_mb < 1000 else "‚ö†Ô∏è High")
    memory_table.add_row("üíæ Available", f"{available_mb:.1f} MB", "üü¢ Plenty" if available_mb > 2000 else "üî¥ Low")
    memory_table.add_row("üìä Efficiency", f"{peak_mb/100:.1f}%", "üåü Great" if peak_mb < 500 else "üí° OK")
    
    console.print(memory_table)
    console.print()
    
    # Summary
    console.print(Panel(
        f"[bold green]üéØ Benchmark Complete![/]\n\n"
        f"[yellow]Model:[/] {Path(results['model_path']).name}\n"
        f"[yellow]Performance:[/] {tokens_per_sec:.1f} tokens/sec\n"
        f"[yellow]Memory:[/] {peak_mb:.1f} MB peak usage\n\n"
        f"[cyan]üí° Tips:[/]\n"
        f"‚Ä¢ Use 4-bit models for better speed\n"
        f"‚Ä¢ Reduce max_tokens for faster inference\n"
        f"‚Ä¢ Monitor memory usage during long sessions",
        title="üèÅ Benchmark Summary",
        border_style="green"
    ))


@main.command()
def update():
    """Update MLX Toolkit to the latest version"""
    console.print("[yellow]To update MLX Toolkit:[/]")
    console.print("1. cd to your mlx-toolkit repository")
    console.print("2. git pull")
    console.print("3. mlx-toolkit-update")
    console.print("\n[cyan]Or run: mlx-toolkit-update (if you have the update script)[/]")


def _detect_and_handle_checkpoints(output_path, target_iters, model_path, data_path):
    """üî• Epic checkpoint detection and resumption system"""
    output_dir = Path(output_path)
    
    # Debug: Show what we're looking for
    console.print(f"[dim]üîç Checking for checkpoints in: {output_path}[/]")
    
    # Look for existing checkpoints
    checkpoints = []
    if output_dir.exists():
        console.print(f"[dim]üìÅ Directory exists, scanning for checkpoint files...[/]")
        
        # Try multiple patterns that MLX-LM might use
        all_files = list(output_dir.glob("*"))
        console.print(f"[dim]üìÑ Found {len(all_files)} files: {[f.name for f in all_files[:10]]}[/]")
        
        # Look for various checkpoint patterns (MLX-LM uses safetensors!)
        checkpoint_patterns = [
            "adapters*.safetensors",     # adapters.safetensors, adapters_100.safetensors
            "*.safetensors",             # any .safetensors file
            "checkpoint*.safetensors",   # checkpoint_100.safetensors
            "model*.safetensors",        # model_100.safetensors
            "**/adapters*.safetensors",  # recursive search
            # Also check for npz just in case
            "adapters*.npz",
            "*.npz",
        ]
        
        checkpoint_files = []
        for pattern in checkpoint_patterns:
            found = list(output_dir.glob(pattern))
            checkpoint_files.extend(found)
            if found:
                console.print(f"[dim]‚úÖ Pattern '{pattern}' found: {[f.name for f in found]}[/]")
        
        # Remove duplicates
        checkpoint_files = list(set(checkpoint_files))
        console.print(f"[dim]üéØ Total unique checkpoint files: {len(checkpoint_files)}[/]")
        
        for ckpt_file in checkpoint_files:
            try:
                # Extract iteration number from filename
                name = ckpt_file.stem
                console.print(f"[dim]üî¨ Analyzing file: {name}[/]")
                
                if name == "adapters":
                    # Final checkpoint
                    iter_num = target_iters
                    console.print(f"[dim]‚úÖ Final checkpoint detected: {name}[/]")
                elif name.startswith("0") and "_adapters" in name:
                    # MLX-LM format: 0000100_adapters.safetensors
                    try:
                        number_part = name.split("_")[0]  # Get "0000100" part
                        iter_num = int(number_part)
                        console.print(f"[dim]‚úÖ MLX-LM checkpoint detected: {name} -> iteration {iter_num}[/]")
                    except ValueError:
                        console.print(f"[dim]‚ùå Could not parse MLX-LM iteration from: {name}[/]")
                        continue
                elif "_" in name and "adapters" in name:
                    # Alternative format: adapters_100.safetensors or other variations
                    parts = name.split("_")
                    try:
                        # Look for numeric parts
                        for part in parts:
                            if part.isdigit():
                                iter_num = int(part)
                                console.print(f"[dim]‚úÖ Numbered checkpoint detected: {name} -> iteration {iter_num}[/]")
                                break
                        else:
                            raise ValueError("No numeric part found")
                    except ValueError:
                        console.print(f"[dim]‚ùå Could not parse iteration from: {name}[/]")
                        continue
                else:
                    # Try to find any numbers in the filename
                    import re
                    numbers = re.findall(r'\d+', name)
                    if numbers:
                        iter_num = int(numbers[0])  # Use the first number found (likely iteration)
                        console.print(f"[dim]‚úÖ Number found in filename: {name} -> iteration {iter_num}[/]")
                    else:
                        console.print(f"[dim]‚ùå No iteration number found in: {name}[/]")
                        continue
                    
                checkpoints.append({
                    'file': ckpt_file,
                    'iteration': iter_num,
                    'name': name,
                    'size': ckpt_file.stat().st_size,
                    'modified': ckpt_file.stat().st_mtime
                })
            except (ValueError, IndexError) as e:
                console.print(f"[dim]‚ùå Error processing {ckpt_file.name}: {e}[/]")
                continue
    else:
        console.print(f"[dim]üìÅ Directory does not exist: {output_path}[/]")
    
    if not checkpoints:
        console.print("[yellow]üîç No valid checkpoints detected - starting fresh training[/]")
        console.print(f"[dim]üí° Checkpoints will be saved to: {output_path}[/]")
        return None
    
    # Sort by iteration number (descending)
    checkpoints.sort(key=lambda x: x['iteration'], reverse=True)
    latest_checkpoint = checkpoints[0]
    
    # Smart completion detection
    # If we have a final "adapters" checkpoint AND numbered checkpoints, 
    # we need to determine if training actually completed or just saved final state
    final_adapters = [c for c in checkpoints if c['name'] == "adapters"]
    numbered_checkpoints = [c for c in checkpoints if c['name'] != "adapters"]
    
    if final_adapters and numbered_checkpoints:
        # We have both final and numbered checkpoints
        highest_numbered = max(numbered_checkpoints, key=lambda x: x['iteration'])
        final_checkpoint = final_adapters[0]
        
        # If the final checkpoint claims to be at target_iters, but we have numbered checkpoints
        # that are much lower, probably use the numbered checkpoint for resumption
        if (final_checkpoint['iteration'] >= target_iters and 
            highest_numbered['iteration'] < target_iters * 0.9):  # More than 10% away
            
            console.print(Panel(
                f"[bold yellow]ü§î Ambiguous Training State Detected[/]\n\n"
                f"Found final checkpoint: [yellow]adapters{final_checkpoint['file'].suffix}[/] (claims {final_checkpoint['iteration']} iterations)\n"
                f"Latest numbered checkpoint: [yellow]{highest_numbered['name']}{highest_numbered['file'].suffix}[/] ({highest_numbered['iteration']} iterations)\n"
                f"Target iterations: [cyan]{target_iters}[/]\n\n"
                f"[green]Recommending resumption from numbered checkpoint ({highest_numbered['iteration']} iterations)[/]",
                title="üîÑ Smart Resumption Decision",
                border_style="yellow"
            ))
            
            # Use the numbered checkpoint for resumption
            latest_checkpoint = highest_numbered
            
        elif final_checkpoint['iteration'] >= target_iters:
            # Training is actually completed
            console.print(Panel(
                f"[bold green]‚úÖ Training already completed![/]\n\n"
                f"Found final checkpoint: [yellow]{final_checkpoint['name']}{final_checkpoint['file'].suffix}[/]\n"
                f"Iterations: [cyan]{final_checkpoint['iteration']}/{target_iters}[/]\n"
                f"Size: [dim]{final_checkpoint['size'] / 1024 / 1024:.1f} MB[/]\n\n"
                f"[cyan]Your LoRA adapters are ready to use![/]\n"
                f"Chat command: [white]mlx-toolkit chat {model_path} --adapter-path {output_path}[/]",
                title="üéâ Training Complete",
                border_style="green"
            ))
            return {'completed': True}
    
    elif latest_checkpoint['iteration'] >= target_iters:
        # Simple case: single checkpoint that reached target
        checkpoint_file = latest_checkpoint['file']
        file_extension = checkpoint_file.suffix
        
        console.print(Panel(
            f"[bold green]‚úÖ Training already completed![/]\n\n"
            f"Found final checkpoint: [yellow]{latest_checkpoint['name']}{file_extension}[/]\n"
            f"Iterations: [cyan]{latest_checkpoint['iteration']}/{target_iters}[/]\n"
            f"Size: [dim]{latest_checkpoint['size'] / 1024 / 1024:.1f} MB[/]\n\n"
            f"[cyan]Your LoRA adapters are ready to use![/]\n"
            f"Chat command: [white]mlx-toolkit chat {model_path} --adapter-path {output_path}[/]",
            title="üéâ Training Complete",
            border_style="green"
        ))
        return {'completed': True}
    
    # Show beautiful checkpoint browser
    console.print(Panel(
        f"[bold yellow]üîÑ Interrupted Training Detected![/]\n\n"
        f"Found {len(checkpoints)} checkpoint(s) in [cyan]{output_path}[/]",
        title="Checkpoint Detection",
        border_style="yellow"
    ))
    
    # Create checkpoint table
    table = Table(title="üìÅ Available Checkpoints", show_header=True, header_style="bold cyan")
    table.add_column("üè∑Ô∏è Name", style="cyan")
    table.add_column("üìä Iteration", style="yellow", justify="right")
    table.add_column("üìà Progress", style="green", justify="right")
    table.add_column("üíæ Size", style="blue", justify="right")
    table.add_column("‚è∞ Modified", style="dim")
    
    for i, ckpt in enumerate(checkpoints[:5]):  # Show top 5
        progress = (ckpt['iteration'] / target_iters) * 100
        size_mb = ckpt['size'] / 1024 / 1024
        modified_time = datetime.fromtimestamp(ckpt['modified']).strftime("%m/%d %H:%M")
        
        # Highlight the latest (recommended)
        name_style = "[bold green]" if i == 0 else ""
        progress_bar = "‚ñà" * int(progress // 10) + "‚ñë" * (10 - int(progress // 10))
        
        table.add_row(
            f"{name_style}{ckpt['name']}{ckpt['file'].suffix}",
            f"{ckpt['iteration']:,}",
            f"{progress:.1f}% {progress_bar}",
            f"{size_mb:.1f} MB",
            modified_time
        )
    
    console.print(table)
    
    # Smart auto-resumption decision (use the checkpoint chosen by smart logic above)
    remaining_iters = target_iters - latest_checkpoint['iteration']
    time_saved = (latest_checkpoint['iteration'] / target_iters) * 100
    
    console.print(Panel(
        f"[bold green]üöÄ Smart Resumption Recommendation[/]\n\n"
        f"Latest checkpoint: [yellow]{latest_checkpoint['name']}{latest_checkpoint['file'].suffix}[/]\n"
        f"Progress: [cyan]{latest_checkpoint['iteration']:,}/{target_iters:,} iterations ({time_saved:.1f}%)[/]\n"
        f"Remaining: [yellow]{remaining_iters:,} iterations[/]\n"
        f"Time saved: [green]~{time_saved:.0f}% of training time![/]\n\n"
        f"[bold cyan]Automatically resuming from latest checkpoint...[/]",
        title="‚ö° Resumption Plan",
        border_style="cyan"
    ))
    
    return {
        'resume_from': str(latest_checkpoint['file']),
        'checkpoint_name': latest_checkpoint['name'],
        'iteration': latest_checkpoint['iteration'],
        'progress': time_saved,
        'remaining': remaining_iters
    }


@main.command()
@click.option("--path", default=None, help="Path to search for checkpoints (auto-detects if not specified)")
@click.option("--debug", is_flag=True, help="Show detailed debugging information")
def checkpoints(path, debug):
    """üîç Browse and manage training checkpoints"""
    console.print("\n")
    console.print(Panel(
        Align.center("üîç [bold cyan]CHECKPOINT BROWSER[/] üîç"),
        style="bold cyan",
        border_style="cyan"
    ))
    
    # Auto-detect checkpoint directories if no path specified
    if path is None:
        search_paths = [
            Path("models/lora_adapters"),  # Default location
            Path("models"),                # Models directory
            Path("."),                     # Current directory
        ]
        
        console.print("[dim]üîç Auto-detecting checkpoint directories...[/]")
        
        all_checkpoint_dirs = []
        for search_path in search_paths:
            if search_path.exists():
                console.print(f"[dim]üìÅ Scanning: {search_path}[/]")
                # Look for directories with checkpoint files
                for item in search_path.iterdir():
                    if item.is_dir():
                        checkpoints_found = list(item.glob("*.safetensors")) + list(item.glob("*.npz"))
                        if checkpoints_found:
                            all_checkpoint_dirs.append(item)
                            console.print(f"[dim]‚úÖ Found checkpoints in: {item}[/]")
        
        if not all_checkpoint_dirs:
            console.print(Panel(
                "[yellow]No checkpoint directories found in common locations![/]\n\n"
                "Searched:\n"
                "‚Ä¢ models/lora_adapters/\n"
                "‚Ä¢ models/\n"
                "‚Ä¢ Current directory\n\n"
                "Use --path to specify a custom location:\n"
                "[cyan]mlx-toolkit checkpoints --path /path/to/your/checkpoints[/]",
                title="üîç No Checkpoints Found",
                border_style="yellow"
            ))
            return
        
        # Show all found directories
        console.print(f"\n[green]‚úÖ Found {len(all_checkpoint_dirs)} checkpoint directories[/]")
        for dir_path in all_checkpoint_dirs:
            console.print(f"[cyan]üìÇ {dir_path}[/]")
        
        # Show summary table for all found directories
        table = Table(title="üìÅ Checkpoint Directories", show_header=True, header_style="bold green")
        table.add_column("üìÇ Directory", style="cyan")
        table.add_column("üìä Checkpoints", style="yellow", justify="right")
        table.add_column("üìà Latest", style="green")
        table.add_column("üíæ Total Size", style="blue", justify="right")
        table.add_column("üè∑Ô∏è Status", style="magenta")
        
        for dir_path in all_checkpoint_dirs:
            checkpoints_found = list(dir_path.glob("*.safetensors")) + list(dir_path.glob("*.npz"))
            size_mb = sum(c.stat().st_size for c in checkpoints_found) / 1024 / 1024
            latest_time = datetime.fromtimestamp(max(checkpoints_found, key=lambda x: x.stat().st_mtime).stat().st_mtime).strftime("%m/%d %H:%M")
            
            # Determine status
            if len(checkpoints_found) == 1:
                status = "üéØ Final"
            elif len(checkpoints_found) > 5:
                status = "üî• Active"
            else:
                status = "üìä Progress"
            
            table.add_row(
                dir_path.name,
                str(len(checkpoints_found)),
                latest_time,
                f"{size_mb:.1f} MB",
                status
            )
        
        console.print(table)
        console.print("\n")
        
        # Use the most recent one for detailed analysis
        search_path = max(all_checkpoint_dirs, key=lambda x: x.stat().st_mtime)
        console.print(f"[bold green]üìã Detailed analysis of {search_path.name}:[/]")
        
        # Show detailed checkpoint analysis for this directory
        _detect_and_handle_checkpoints(str(search_path), 1000, "example-model", "example-data")
        return
        
    else:
        search_path = Path(path)
    
    checkpoint_dirs = []
    
    if search_path.exists():
        # Look for directories with checkpoints (safetensors or npz) - this is for specific path case
        for item in search_path.iterdir():
            if item.is_dir():
                checkpoints_found = list(item.glob("*.safetensors")) + list(item.glob("*.npz"))
                if checkpoints_found:
                    checkpoint_dirs.append({
                        'dir': item,
                        'checkpoints': len(checkpoints_found),
                        'latest': max(checkpoints_found, key=lambda x: x.stat().st_mtime),
                        'total_size': sum(c.stat().st_size for c in checkpoints_found)
                    })
    
    if not checkpoint_dirs:
        console.print(Panel(
            f"[yellow]No checkpoints found in {path}[/]\n\n"
            f"Checkpoints are created during LoRA training with:\n"
            f"[cyan]mlx-toolkit lora <model> <data>[/]\n\n"
            f"They allow you to resume interrupted training!",
            title="üîç No Checkpoints",
            border_style="yellow"
        ))
        return
    
    # Show checkpoint directories
    table = Table(title="üìÅ Checkpoint Directories", show_header=True, header_style="bold green")
    table.add_column("üìÇ Directory", style="cyan")
    table.add_column("üìä Checkpoints", style="yellow", justify="right")
    table.add_column("üìà Latest", style="green")
    table.add_column("üíæ Total Size", style="blue", justify="right")
    table.add_column("üè∑Ô∏è Status", style="magenta")
    
    for info in checkpoint_dirs:
        size_mb = info['total_size'] / 1024 / 1024
        latest_time = datetime.fromtimestamp(info['latest'].stat().st_mtime).strftime("%m/%d %H:%M")
        
        # Determine status
        if info['checkpoints'] == 1:
            status = "üéØ Final"
        elif info['checkpoints'] > 5:
            status = "üî• Active"
        else:
            status = "üìä Progress"
        
        table.add_row(
            info['dir'].name,
            str(info['checkpoints']),
            latest_time,
            f"{size_mb:.1f} MB",
            status
        )
    
    console.print(table)
    console.print("\n")
    
    # Show detailed view of first directory
    if checkpoint_dirs:
        first_dir = checkpoint_dirs[0]
        console.print(f"[bold green]üìã Detailed view of {first_dir['dir'].name}:[/]")
        
        # Use the same detection logic to show details
        _detect_and_handle_checkpoints(str(first_dir['dir']), 1000, "example-model", "example-data")


@main.command()
@click.argument("directory", required=False)
@click.option("--target-iters", default=1000, help="Target iterations for analysis")
def debug_checkpoints(directory, target_iters):
    """üîß Debug checkpoint detection (for troubleshooting)"""
    
    # Auto-detect directory if not specified
    if not directory:
        # Look for the most recent LoRA output directory
        search_paths = [
            Path("models/lora_adapters"),
            Path("models"),
            Path("."),
        ]
        
        checkpoint_dirs = []
        for search_path in search_paths:
            if search_path.exists():
                for item in search_path.iterdir():
                    if item.is_dir():
                        checkpoints_found = list(item.glob("*.safetensors")) + list(item.glob("*.npz"))
                        if checkpoints_found:
                            checkpoint_dirs.append(item)
        
        if not checkpoint_dirs:
            console.print(f"\n[red]‚ùå No checkpoint directories found![/]")
            console.print(f"[yellow]Please specify a directory:[/] mlx-toolkit debug-checkpoints /path/to/checkpoints")
            return
        
        # Use the most recent one
        directory = str(max(checkpoint_dirs, key=lambda x: x.stat().st_mtime))
        console.print(f"[dim]üîç Auto-detected most recent checkpoint directory: {directory}[/]")
    
    # Run the detection with full debugging
    result = _detect_and_handle_checkpoints(directory, target_iters, "debug-model", "debug-data")
    
    if result:
        console.print(f"\n[bold green]‚úÖ Detection Result:[/]")
        console.print(f"Resume from: [yellow]{result.get('resume_from', 'N/A')}[/]")
        console.print(f"Checkpoint name: [yellow]{result.get('checkpoint_name', 'N/A')}[/]")
        console.print(f"Progress: [yellow]{result.get('progress', 0):.1f}%[/]")
    else:
        console.print(f"\n[red]‚ùå No checkpoints detected[/]")


if __name__ == "__main__":
    main()